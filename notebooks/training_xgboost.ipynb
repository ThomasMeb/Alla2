{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns=200\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/btc_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, model, features, target, window_size):\n",
    "      \n",
    "      # Initialiser les listes pour stocker les prédictions et les vraies valeurs\n",
    "      predictions = []\n",
    "      actuals = []\n",
    "\n",
    "      # Boucle à travers les données de la taille de la fenêtre jusqu'à la fin des données\n",
    "      for i in range(window_size, len(data) - 1):\n",
    "            # Diviser les données en ensembles d'entraînement et de test\n",
    "            X_train = features.iloc[i-window_size:i, :]\n",
    "            y_train = target.iloc[i-window_size:i]\n",
    "            X_test = features.iloc[i:i+1, :]\n",
    "            y_test = target.iloc[i]\n",
    "\n",
    "            #Normaliser les données\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # Entraîner un modèle\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Faire une prédiction\n",
    "            prediction = model.predict(X_test)[0]\n",
    "            \n",
    "            # Stocker les prédictions et les vraies valeurs\n",
    "            predictions.append(prediction)\n",
    "            actuals.append(y_test)\n",
    "\n",
    "      # Évaluer le modèle\n",
    "      accuracy = accuracy_score(actuals, predictions)\n",
    "      print(f'Model Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earn_metric(predicted_probs, progressions, n_days, i):\n",
    "      base = c = 1\n",
    "      for j in range(n_days):\n",
    "            index = len(predicted_probs) - n_days + j\n",
    "            c *= predicted_probs[index] * progressions[index] + (1 - predicted_probs[index])\n",
    "            base *= progressions[index]\n",
    "      return c / base\n",
    "\n",
    "\n",
    "def train_model_proba_metric(data, model, features, target, window_size, n_days):\n",
    "\n",
    "      # Initialiser les listes pour stocker les probaabilités prédites et les vraies valeurs\n",
    "      predicted_probs = []\n",
    "      progressions = []\n",
    "      metric = []\n",
    "\n",
    "      # Boucle à travers les données de la taille de la fenêtre jusqu'à la fin des données\n",
    "      for i in range(window_size, len(data) - 1):\n",
    "            # Diviser les données en ensembles d'entraînement et de test\n",
    "            X_train = features.iloc[i-window_size:i, :]\n",
    "            y_train = target.iloc[i-window_size:i]\n",
    "            X_test = features.iloc[i:i+1, :]\n",
    "            y_test = target.iloc[i]\n",
    "\n",
    "            # Normaliser les données\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # Entraîner un modèle\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Obtenir les probabilités prédites pour la classe positive\n",
    "            prediction_prob = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Stocker les probabilités prédites et les vraies valeurs\n",
    "            predicted_probs.extend(prediction_prob)\n",
    "            \n",
    "            # Récupérer la progression réelle\n",
    "            progressions.append(data.iloc[i]['progression tomorrow']+1)\n",
    "\n",
    "            if i >= window_size + n_days:\n",
    "                  metric.append(earn_metric(predicted_probs, progressions, n_days, i))\n",
    "\n",
    "            # Sauvegardez les modèles dans un fichier\n",
    "            filename = f'../models/xgboost_models/xgboost_{data.index[i]}.pkl'\n",
    "            pickle.dump(model, open(filename, 'wb'))\n",
    "      \n",
    "      return np.mean(metric) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les caractéristiques et exclure la dernière ligne\n",
    "features = data.drop(columns=['progression tomorrow', 'target', 'close', 'high', 'low', 'volumefrom', 'market_cap', 'difficulty']).iloc[:-1, :]\n",
    "target = data['target'].iloc[:-1]\n",
    "window_size = 1500\n",
    "n_days = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_xgb = {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
    "\n",
    "xgboost = XGBClassifier(**best_params_xgb, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.002236645020043"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_proba_metric(data, xgboost, features, target, window_size, n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = {\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.5887799564270153\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5833333333333334\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.5999455337690632\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5923202614379085\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.593681917211329\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5874183006535948\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.6029411764705882\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5925925925925926\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.5980392156862745\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5912309368191722\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.6013071895424836\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5863289760348583\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.5980392156862745\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5895969498910676\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.5923202614379085\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5833333333333334\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.585511982570806\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5827886710239651\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.6029411764705882\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5906862745098039\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.5996732026143791\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5893246187363834\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.6056644880174292\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5944989106753813\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.6043028322440087\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5944989106753813\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.6002178649237473\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5874183006535948\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.5999455337690632\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5885076252723311\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.6015795206971678\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5841503267973857\"\n",
      "Best Model Accuracy: 60.56645%\n",
      "Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='sklearn')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "# Créer une liste de combinaisons de paramètres\n",
    "grid_list = list(ParameterGrid(param_grid_xgb))\n",
    "\n",
    "# Pour boucler sur chaque combinaison :\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "for params in grid_list:\n",
    "    try:\n",
    "        # Instancier le modèle avec les paramètres\n",
    "        model_instance = XGBClassifier(**params)\n",
    "        \n",
    "        # Appliquer la fonction train_model\n",
    "        accuracy = train_model_grid(data, model_instance, features, target, window_size)\n",
    "        \n",
    "        # Si le modèle actuel a une meilleure précision que le précédent meilleur modèle, stocker sa précision et ses paramètres\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = params\n",
    "            print(f\"Nouveaux meilleurs paramètres trouvés : {params}, accuracy : {accuracy}\")\n",
    "\n",
    "        else:\n",
    "            print(f'Trop nul la honte : {params}, accuracy : {accuracy}\"')\n",
    "\n",
    "        \n",
    "            \n",
    "    except Exception as e:\n",
    "        # Gérer les combinaisons de paramètres non compatibles\n",
    "        error_message = f\"Error with parameters {params}: {e}\"\n",
    "        \n",
    "\n",
    "print(f\"Best Model Accuracy: {best_accuracy * 100:.5f}%\")\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.0084009335278077\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.0083829500965855\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0205983479391452\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.020363818651446\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.013274166001476\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.0135417707667784\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0282829939921057\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.0289380501062246\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.0492214962034623\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.0461964906045014\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0598968818000603\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.0564741754830942\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.0554043480703776\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.0564663984056746\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0652933395745423\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.0678019107384453\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.0129949662206572\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.012821216645928\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0242204834996174\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.0232371985916304\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.0197043519555151\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.0205513204866385\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0338016611112675\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.034420523759096\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.0518600748332796\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.0475903178455197\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0626972479357284\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.0578104539399624\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.063850805826207\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.0592964781423453\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0727029942679231\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.0677705446894292\"\n",
      "Best Earnings: 107.27030%\n",
      "Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='sklearn')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "# Créer une liste de combinaisons de paramètres\n",
    "grid_list = list(ParameterGrid(param_grid_xgb))\n",
    "\n",
    "# Pour boucler sur chaque combinaison :\n",
    "best_earning = 0\n",
    "best_params = None\n",
    "\n",
    "for params in grid_list:\n",
    "    try:\n",
    "        # Instancier le modèle avec les paramètres\n",
    "        model_instance = XGBClassifier(**params, random_state = 42)\n",
    "        \n",
    "        # Appliquer la fonction train_model\n",
    "        earning = train_model_proba_metric(data, model_instance, features, target, window_size, n_days)\n",
    "        \n",
    "        # Si le modèle actuel a une meilleure précision que le précédent meilleur modèle, stocker sa précision et ses paramètres\n",
    "        if earning > best_earning:\n",
    "            best_earning = earning\n",
    "            best_params = params\n",
    "            print(f\"Nouveaux meilleurs paramètres trouvés : {params}, earning : {earning}\")\n",
    "\n",
    "        else:\n",
    "            print(f'Trop nul la honte : {params}, earning : {earning}')\n",
    "\n",
    "        \n",
    "            \n",
    "    except Exception as e:\n",
    "        # Gérer les combinaisons de paramètres non compatibles\n",
    "        error_message = f\"Error with parameters {params}: {e}\"\n",
    "        \n",
    "\n",
    "print(f\"Best Earnings: {best_earning * 100:.5f}%\")\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
