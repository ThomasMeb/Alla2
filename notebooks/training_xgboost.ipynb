{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns=200\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/btc_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, model, features, target, window_size):\n",
    "      \n",
    "      # Initialiser les listes pour stocker les prédictions et les vraies valeurs\n",
    "      predictions = []\n",
    "      actuals = []\n",
    "\n",
    "      # Boucle à travers les données de la taille de la fenêtre jusqu'à la fin des données\n",
    "      for i in range(window_size, len(data) - 1):\n",
    "            # Diviser les données en ensembles d'entraînement et de test\n",
    "            X_train = features.iloc[i-window_size:i, :]\n",
    "            y_train = target.iloc[i-window_size:i]\n",
    "            X_test = features.iloc[i:i+1, :]\n",
    "            y_test = target.iloc[i]\n",
    "\n",
    "            #Normaliser les données\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # Entraîner un modèle\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Faire une prédiction\n",
    "            prediction = model.predict(X_test)[0]\n",
    "            \n",
    "            # Stocker les prédictions et les vraies valeurs\n",
    "            predictions.append(prediction)\n",
    "            actuals.append(y_test)\n",
    "\n",
    "      # Évaluer le modèle\n",
    "      accuracy = accuracy_score(actuals, predictions)\n",
    "      print(f'Model Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earn_metric(predicted_probs, progressions, n_days, current_index):\n",
    "    # Assurer qu'il y a assez de données pour calculer la métrique\n",
    "    if current_index < n_days - 1:\n",
    "        return None\n",
    "\n",
    "    base = c = 1\n",
    "    for j in range(current_index - n_days + 1, current_index + 1):\n",
    "        c *= predicted_probs[j] * progressions[j] + (1 - predicted_probs[j])\n",
    "        base *= progressions[j]\n",
    "    return c / base\n",
    "\n",
    "\n",
    "\n",
    "def train_model_proba_metric_save(data, model, features, target, window_size, n_days):\n",
    "\n",
    "\n",
    "      # Initialiser les listes pour stocker les probaabilités prédites et les vraies valeurs\n",
    "      predicted_probs = []\n",
    "      progressions = []\n",
    "      metric = []\n",
    "\n",
    "      # Boucle à travers les données de la taille de la fenêtre jusqu'à la fin des données\n",
    "      for i in range(window_size, len(data) - 1):\n",
    "            # Diviser les données en ensembles d'entraînement et de test\n",
    "            X_train = features.iloc[i-window_size:i, :]\n",
    "            y_train = target.iloc[i-window_size:i]\n",
    "            X_test = features.iloc[i:i+1, :]\n",
    "            y_test = target.iloc[i]\n",
    "\n",
    "            # Normaliser les données\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # Entraîner un modèle\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Obtenir les probabilités prédites pour la classe positive\n",
    "            prediction_prob = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Stocker les probabilités prédites et les vraies valeurs\n",
    "            predicted_probs.extend(prediction_prob)\n",
    "            \n",
    "            # Récupérer la progression réelle\n",
    "            progressions.append(data.iloc[i]['progression tomorrow']+1)\n",
    "\n",
    "            if i >= window_size + n_days:\n",
    "                  metric.append(earn_metric(predicted_probs, progressions, n_days, i))\n",
    "\n",
    "            # Sauvegarder le modèle et le scaler dans un fichier\n",
    "            model_filename = f'../models/xgboost_models/xgboost_{data.index[i]}.pkl'\n",
    "            scaler_filename = f'../scalers/scaler_{data.index[i]}.pkl'\n",
    "            pickle.dump(model, open(model_filename, 'wb'))\n",
    "            pickle.dump(scaler, open(scaler_filename, 'wb'))\n",
    "      \n",
    "      return np.mean(metric) \n",
    "\n",
    "def train_model_proba_metric(data, model, features, target, window_size, n_days):\n",
    "\n",
    "      # Initialiser les listes pour stocker les probaabilités prédites et les vraies valeurs\n",
    "      predicted_probs = []\n",
    "      progressions = []\n",
    "      metric = []\n",
    "\n",
    "      # Boucle à travers les données de la taille de la fenêtre jusqu'à la fin des données\n",
    "      for i in range(window_size, len(data) - 1):\n",
    "            # Diviser les données en ensembles d'entraînement et de test\n",
    "            X_train = features.iloc[i-window_size:i, :]\n",
    "            y_train = target.iloc[i-window_size:i]\n",
    "            X_test = features.iloc[i:i+1, :]\n",
    "            y_test = target.iloc[i]\n",
    "\n",
    "            # Normaliser les données\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # Entraîner un modèle\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Obtenir les probabilités prédites pour la classe positive\n",
    "            prediction_prob = model.predict_proba(X_test)[:, 1]\n",
    "            \n",
    "            # Stocker les probabilités prédites et les vraies valeurs\n",
    "            predicted_probs.extend(prediction_prob)\n",
    "            \n",
    "            # Récupérer la progression réelle\n",
    "            progressions.append(data.iloc[i]['progression tomorrow']+1)\n",
    "\n",
    "            if i >= window_size + n_days:\n",
    "                  metric.append(earn_metric(predicted_probs, progressions, n_days, i))\n",
    "      \n",
    "      return np.mean(metric) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cumulative_model(data, model, features, target, window_size, test_size, n_days):\n",
    "    # Initialiser les listes pour stocker les scores de performance\n",
    "    performance_scores = []\n",
    "\n",
    "    # StandardScaler pour normaliser les features\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Initialiser l'ensemble d'entraînement\n",
    "    X_train = features.iloc[:window_size, :]\n",
    "    y_train = target.iloc[:window_size]\n",
    "\n",
    "    # Listes pour stocker toutes les prédictions et progressions\n",
    "    all_predictions = []\n",
    "    all_progressions = []\n",
    "\n",
    "    # Boucle à travers les données, en augmentant la taille de l'ensemble d'entraînement à chaque itération\n",
    "    for i in range(window_size, len(data) - test_size):\n",
    "        # Diviser les données en ensembles d'entraînement et de test\n",
    "        X_test = features.iloc[i:i + test_size, :]\n",
    "        y_test = target.iloc[i:i + test_size]\n",
    "\n",
    "        # Normaliser les données\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Entraîner le modèle\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Faire des prédictions sur le jeu de test\n",
    "        prediction = model.predict(X_test_scaled)[0]  # Prédiction pour un seul jour\n",
    "        progression = data.iloc[i]['progression tomorrow'] + 1\n",
    "\n",
    "        all_predictions.append(prediction)\n",
    "        all_progressions.append(progression)\n",
    "\n",
    "        # Appliquer la métrique si on a suffisamment de données\n",
    "        if i >= window_size + n_days - 1:\n",
    "            score = earn_metric(all_predictions, all_progressions, n_days, i - window_size)\n",
    "            if score is not None:\n",
    "                performance_scores.append(score)\n",
    "\n",
    "        # Ajouter de nouvelles données à l'ensemble d'entraînement pour la prochaine itération\n",
    "        X_train = features.iloc[:i + test_size, :]\n",
    "        y_train = target.iloc[:i + test_size]\n",
    "\n",
    "    # Calculer la performance moyenne\n",
    "    average_performance = np.mean(performance_scores)\n",
    "\n",
    "    # Sauvegarder le modèle et le scaler final\n",
    "    pickle.dump(model, open('../models/final_model.pkl', 'wb'))\n",
    "    pickle.dump(scaler, open('../scalers/final_scaler.pkl', 'wb'))\n",
    "\n",
    "    return average_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les caractéristiques et exclure la dernière ligne\n",
    "features = data.drop(columns=['progression tomorrow', 'target', 'close', 'high', 'low', 'volumefrom', 'market_cap', 'difficulty']).iloc[:-1, :]\n",
    "target = data['target'].iloc[:-1]\n",
    "window_size = 1500\n",
    "test_size = 1\n",
    "n_days = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_xgb = {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
    "\n",
    "xgboost = XGBClassifier(**best_params_xgb, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.003973437273559"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cumulative_model(data, xgboost, features, target, window_size, test_size, n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.002236645020043"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_proba_metric_save(data, xgboost, features, target, window_size, n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volumefrom</th>\n",
       "      <th>volumeto</th>\n",
       "      <th>close</th>\n",
       "      <th>progression daily</th>\n",
       "      <th>progression tomorrow</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-12-10</th>\n",
       "      <td>0.2040</td>\n",
       "      <td>0.1825</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>1684.25</td>\n",
       "      <td>3.281700e+02</td>\n",
       "      <td>0.2040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-11</th>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.1907</td>\n",
       "      <td>0.2040</td>\n",
       "      <td>5788.69</td>\n",
       "      <td>1.267200e+03</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-12</th>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2068</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>1963.56</td>\n",
       "      <td>4.188000e+02</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>-0.035088</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-13</th>\n",
       "      <td>0.2300</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>6415.59</td>\n",
       "      <td>1.425920e+03</td>\n",
       "      <td>0.2299</td>\n",
       "      <td>-0.035088</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-14</th>\n",
       "      <td>0.2468</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.2299</td>\n",
       "      <td>10388.10</td>\n",
       "      <td>2.403830e+03</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.073075</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-10</th>\n",
       "      <td>37536.4400</td>\n",
       "      <td>36342.2700</td>\n",
       "      <td>36704.1400</td>\n",
       "      <td>32545.30</td>\n",
       "      <td>1.206041e+09</td>\n",
       "      <td>37321.5100</td>\n",
       "      <td>0.029906</td>\n",
       "      <td>0.016820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-11</th>\n",
       "      <td>37417.2900</td>\n",
       "      <td>36701.8300</td>\n",
       "      <td>37321.5100</td>\n",
       "      <td>14920.50</td>\n",
       "      <td>5.532155e+08</td>\n",
       "      <td>37142.2200</td>\n",
       "      <td>0.016820</td>\n",
       "      <td>-0.004804</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-12</th>\n",
       "      <td>37233.4500</td>\n",
       "      <td>36747.0500</td>\n",
       "      <td>37142.2200</td>\n",
       "      <td>9320.87</td>\n",
       "      <td>3.455680e+08</td>\n",
       "      <td>37079.7600</td>\n",
       "      <td>-0.004804</td>\n",
       "      <td>-0.001682</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-13</th>\n",
       "      <td>37429.5200</td>\n",
       "      <td>36358.0000</td>\n",
       "      <td>37079.7600</td>\n",
       "      <td>24368.53</td>\n",
       "      <td>8.976713e+08</td>\n",
       "      <td>36482.5100</td>\n",
       "      <td>-0.001682</td>\n",
       "      <td>-0.016107</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-14</th>\n",
       "      <td>36752.3100</td>\n",
       "      <td>35901.0200</td>\n",
       "      <td>36482.5100</td>\n",
       "      <td>17534.52</td>\n",
       "      <td>6.383129e+08</td>\n",
       "      <td>36356.8100</td>\n",
       "      <td>-0.016107</td>\n",
       "      <td>-0.003445</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4723 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  high         low        open  volumefrom      volumeto  \\\n",
       "time                                                                       \n",
       "2010-12-10      0.2040      0.1825      0.2000     1684.25  3.281700e+02   \n",
       "2010-12-11      0.2280      0.1907      0.2040     5788.69  1.267200e+03   \n",
       "2010-12-12      0.2280      0.2068      0.2280     1963.56  4.188000e+02   \n",
       "2010-12-13      0.2300      0.2100      0.2200     6415.59  1.425920e+03   \n",
       "2010-12-14      0.2468      0.2100      0.2299    10388.10  2.403830e+03   \n",
       "...                ...         ...         ...         ...           ...   \n",
       "2023-11-10  37536.4400  36342.2700  36704.1400    32545.30  1.206041e+09   \n",
       "2023-11-11  37417.2900  36701.8300  37321.5100    14920.50  5.532155e+08   \n",
       "2023-11-12  37233.4500  36747.0500  37142.2200     9320.87  3.455680e+08   \n",
       "2023-11-13  37429.5200  36358.0000  37079.7600    24368.53  8.976713e+08   \n",
       "2023-11-14  36752.3100  35901.0200  36482.5100    17534.52  6.383129e+08   \n",
       "\n",
       "                 close  progression daily  progression tomorrow  target  \n",
       "time                                                                     \n",
       "2010-12-10      0.2040                NaN              0.020000       1  \n",
       "2010-12-11      0.2280           0.020000              0.117647       1  \n",
       "2010-12-12      0.2200           0.117647             -0.035088       0  \n",
       "2010-12-13      0.2299          -0.035088              0.045000       1  \n",
       "2010-12-14      0.2467           0.045000              0.073075       1  \n",
       "...                ...                ...                   ...     ...  \n",
       "2023-11-10  37321.5100           0.029906              0.016820       1  \n",
       "2023-11-11  37142.2200           0.016820             -0.004804       0  \n",
       "2023-11-12  37079.7600          -0.004804             -0.001682       0  \n",
       "2023-11-13  36482.5100          -0.001682             -0.016107       0  \n",
       "2023-11-14  36356.8100          -0.016107             -0.003445       0  \n",
       "\n",
       "[4723 rows x 9 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw = pd.read_csv('../data/data_raw.csv', index_col=0)\n",
    "data_raw['target'] = np.where(data_raw['progression tomorrow'] > 0, 1, 0)\n",
    "start_date = pd.to_datetime(\"2011-01-01\")\n",
    "data_raw.index = pd.to_datetime(data_raw.index)\n",
    "data_raw = data_raw[data_raw.index >= start_date]\n",
    "data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les caractéristiques et exclure la dernière ligne\n",
    "features = data_raw.drop(columns=['progression tomorrow', 'target', 'close', 'high', 'low', 'volumefrom']).iloc[:-1, :]\n",
    "target = data['target']\n",
    "window_size = 1500\n",
    "n_days = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_xgb = {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
    "\n",
    "xgboost = XGBClassifier(**best_params_xgb, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997088846261043"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_proba_metric(data_raw, xgboost, features, target, window_size, n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('../data/data_raw.csv', index_col=0)\n",
    "data_raw['target'] = np.where(data_raw['progression tomorrow'] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stoch_osc(btc_data):\n",
    "      # Oscillateur Stochastique\n",
    "\n",
    "      rolling_window = 14\n",
    "\n",
    "      # Trouver le prix le plus bas et le plus élevé sur la période\n",
    "      btc_data['rolling_low'] = btc_data['low'].rolling(window=rolling_window).min()\n",
    "      btc_data['rolling_high'] = btc_data['high'].rolling(window=rolling_window).max()\n",
    "\n",
    "      # Calcul du Stochastic Oscillator\n",
    "      btc_data['k'] = 100 * ((btc_data['open'] - btc_data['rolling_low']) / (btc_data['rolling_high'] - btc_data['rolling_low']))\n",
    "\n",
    "      # Suppression des colonnes intermédiaires\n",
    "      btc_data.drop(columns=['rolling_low', 'rolling_high'], inplace=True)\n",
    "\n",
    "def calculate_momentum(btc_data):\n",
    "      # Momentum\n",
    "\n",
    "      n_days = 10\n",
    "\n",
    "      # Calcul du Momentum\n",
    "      btc_data['momentum'] = btc_data['open'] - btc_data['open'].shift(n_days)\n",
    "\n",
    "def calculate_atr(btc_data):\n",
    "      # Calcul de l'ATR (Average True Range)\n",
    "\n",
    "      # Calculer la différence de prix de clôture par rapport à la journée précédente\n",
    "      btc_data['prev_close'] = btc_data['open'].shift(1)\n",
    "\n",
    "      # Calculer les trois composantes du True Range\n",
    "      btc_data['high_minus_low'] = btc_data['high'] - btc_data['low']\n",
    "      btc_data['high_minus_prev_close'] = abs(btc_data['high'] - btc_data['prev_close'])\n",
    "      btc_data['low_minus_prev_close'] = abs(btc_data['low'] - btc_data['prev_close'])\n",
    "\n",
    "      # Déterminer le True Range comme étant le maximum des trois valeurs précédentes\n",
    "      btc_data['tr'] = btc_data[['high_minus_low', 'high_minus_prev_close', 'low_minus_prev_close']].max(axis=1)\n",
    "\n",
    "      # Calculer l'ATR comme étant la moyenne mobile du TR sur une période de 14 jours\n",
    "      rolling_window = 14\n",
    "      btc_data['atr'] = btc_data['tr'].rolling(window=rolling_window).mean()\n",
    "\n",
    "      # Supprimer les colonnes intermédiaires\n",
    "      columns_to_drop = ['prev_close', 'high_minus_low', 'high_minus_prev_close', 'low_minus_prev_close', 'tr']\n",
    "      btc_data.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "def calculate_rsi(btc_data):\n",
    "      # Calcul du RSI \n",
    "\n",
    "      # Calculer la différence de prix par rapport à la journée précédente\n",
    "      btc_data['delta'] = btc_data['open'].diff()\n",
    "\n",
    "      # Identifier les gains et les pertes\n",
    "      btc_data['gain'] = btc_data['delta'].where(btc_data['delta'] > 0, 0)\n",
    "      btc_data['loss'] = -btc_data['delta'].where(btc_data['delta'] < 0, 0)\n",
    "\n",
    "      # Calculer la moyenne des gains et des pertes sur 14 jours\n",
    "      rolling_window = 14\n",
    "      btc_data['avg_gain'] = btc_data['gain'].rolling(window=rolling_window).mean()\n",
    "      btc_data['avg_loss'] = btc_data['loss'].rolling(window=rolling_window).mean()\n",
    "\n",
    "      # Calculer le RS (Relative Strength)\n",
    "      btc_data['rs'] = btc_data['avg_gain'] / btc_data['avg_loss']\n",
    "\n",
    "      # Calculer le RSI\n",
    "      btc_data['rsi'] = 100 - (100 / (1 + btc_data['rs']))\n",
    "\n",
    "      # Supprimer les colonnes intermédiaires\n",
    "      btc_data.drop(columns=['delta', 'gain', 'loss', 'avg_gain', 'avg_loss', 'rs'], inplace=True)\n",
    "\n",
    "calculate_momentum(data_raw)\n",
    "calculate_stoch_osc(data_raw)\n",
    "calculate_atr(data_raw)\n",
    "calculate_rsi(data_raw)\n",
    "\n",
    "start_date = pd.to_datetime(\"2011-01-01\")\n",
    "data_raw.index = pd.to_datetime(data_raw.index)\n",
    "data_raw = data_raw[data_raw.index >= start_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volumefrom</th>\n",
       "      <th>volumeto</th>\n",
       "      <th>close</th>\n",
       "      <th>progression daily</th>\n",
       "      <th>progression tomorrow</th>\n",
       "      <th>target</th>\n",
       "      <th>momentum</th>\n",
       "      <th>k</th>\n",
       "      <th>atr</th>\n",
       "      <th>rsi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-01</th>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>2821.24</td>\n",
       "      <td>8.419500e+02</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060</td>\n",
       "      <td>98.360656</td>\n",
       "      <td>0.018136</td>\n",
       "      <td>75.041736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-02</th>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>5352.11</td>\n",
       "      <td>1.584660e+03</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050</td>\n",
       "      <td>98.360656</td>\n",
       "      <td>0.018214</td>\n",
       "      <td>74.831650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-03</th>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>1425.19</td>\n",
       "      <td>4.208500e+02</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.016667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050</td>\n",
       "      <td>98.360656</td>\n",
       "      <td>0.016429</td>\n",
       "      <td>75.402884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-04</th>\n",
       "      <td>0.2999</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>1879.00</td>\n",
       "      <td>5.483300e+02</td>\n",
       "      <td>0.2989</td>\n",
       "      <td>-0.016667</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>1</td>\n",
       "      <td>0.047</td>\n",
       "      <td>90.163934</td>\n",
       "      <td>0.015286</td>\n",
       "      <td>64.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-05</th>\n",
       "      <td>0.2990</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.2989</td>\n",
       "      <td>357.16</td>\n",
       "      <td>1.061900e+02</td>\n",
       "      <td>0.2990</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>1</td>\n",
       "      <td>0.049</td>\n",
       "      <td>96.500000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>90.397805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-10</th>\n",
       "      <td>37536.4400</td>\n",
       "      <td>36342.270</td>\n",
       "      <td>36704.1400</td>\n",
       "      <td>32545.30</td>\n",
       "      <td>1.206041e+09</td>\n",
       "      <td>37321.5100</td>\n",
       "      <td>0.029906</td>\n",
       "      <td>0.016820</td>\n",
       "      <td>1</td>\n",
       "      <td>2209.580</td>\n",
       "      <td>68.735235</td>\n",
       "      <td>1173.541429</td>\n",
       "      <td>77.472798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-11</th>\n",
       "      <td>37417.2900</td>\n",
       "      <td>36701.830</td>\n",
       "      <td>37321.5100</td>\n",
       "      <td>14920.50</td>\n",
       "      <td>5.532155e+08</td>\n",
       "      <td>37142.2200</td>\n",
       "      <td>0.016820</td>\n",
       "      <td>-0.004804</td>\n",
       "      <td>0</td>\n",
       "      <td>2663.780</td>\n",
       "      <td>83.451695</td>\n",
       "      <td>1180.479286</td>\n",
       "      <td>84.113197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-12</th>\n",
       "      <td>37233.4500</td>\n",
       "      <td>36747.050</td>\n",
       "      <td>37142.2200</td>\n",
       "      <td>9320.87</td>\n",
       "      <td>3.455680e+08</td>\n",
       "      <td>37079.7600</td>\n",
       "      <td>-0.004804</td>\n",
       "      <td>-0.001682</td>\n",
       "      <td>0</td>\n",
       "      <td>1702.460</td>\n",
       "      <td>78.426595</td>\n",
       "      <td>1160.555714</td>\n",
       "      <td>80.518099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-13</th>\n",
       "      <td>37429.5200</td>\n",
       "      <td>36358.000</td>\n",
       "      <td>37079.7600</td>\n",
       "      <td>24368.53</td>\n",
       "      <td>8.976713e+08</td>\n",
       "      <td>36482.5100</td>\n",
       "      <td>-0.001682</td>\n",
       "      <td>-0.016107</td>\n",
       "      <td>0</td>\n",
       "      <td>2132.610</td>\n",
       "      <td>76.838135</td>\n",
       "      <td>1181.945000</td>\n",
       "      <td>77.550176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-14</th>\n",
       "      <td>36752.3100</td>\n",
       "      <td>35901.020</td>\n",
       "      <td>36482.5100</td>\n",
       "      <td>17534.52</td>\n",
       "      <td>6.383129e+08</td>\n",
       "      <td>36356.8100</td>\n",
       "      <td>-0.016107</td>\n",
       "      <td>-0.003445</td>\n",
       "      <td>0</td>\n",
       "      <td>1750.230</td>\n",
       "      <td>61.269876</td>\n",
       "      <td>1218.451429</td>\n",
       "      <td>69.210402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4701 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  high        low        open  volumefrom      volumeto  \\\n",
       "time                                                                      \n",
       "2011-01-01      0.3000      0.292      0.3000     2821.24  8.419500e+02   \n",
       "2011-01-02      0.3000      0.289      0.3000     5352.11  1.584660e+03   \n",
       "2011-01-03      0.3000      0.290      0.3000     1425.19  4.208500e+02   \n",
       "2011-01-04      0.2999      0.289      0.2950     1879.00  5.483300e+02   \n",
       "2011-01-05      0.2990      0.290      0.2989      357.16  1.061900e+02   \n",
       "...                ...        ...         ...         ...           ...   \n",
       "2023-11-10  37536.4400  36342.270  36704.1400    32545.30  1.206041e+09   \n",
       "2023-11-11  37417.2900  36701.830  37321.5100    14920.50  5.532155e+08   \n",
       "2023-11-12  37233.4500  36747.050  37142.2200     9320.87  3.455680e+08   \n",
       "2023-11-13  37429.5200  36358.000  37079.7600    24368.53  8.976713e+08   \n",
       "2023-11-14  36752.3100  35901.020  36482.5100    17534.52  6.383129e+08   \n",
       "\n",
       "                 close  progression daily  progression tomorrow  target  \\\n",
       "time                                                                      \n",
       "2011-01-01      0.3000           0.000000              0.000000       0   \n",
       "2011-01-02      0.3000           0.000000              0.000000       0   \n",
       "2011-01-03      0.2950           0.000000             -0.016667       0   \n",
       "2011-01-04      0.2989          -0.016667              0.013220       1   \n",
       "2011-01-05      0.2990           0.013220              0.000335       1   \n",
       "...                ...                ...                   ...     ...   \n",
       "2023-11-10  37321.5100           0.029906              0.016820       1   \n",
       "2023-11-11  37142.2200           0.016820             -0.004804       0   \n",
       "2023-11-12  37079.7600          -0.004804             -0.001682       0   \n",
       "2023-11-13  36482.5100          -0.001682             -0.016107       0   \n",
       "2023-11-14  36356.8100          -0.016107             -0.003445       0   \n",
       "\n",
       "            momentum          k          atr        rsi  \n",
       "time                                                     \n",
       "2011-01-01     0.060  98.360656     0.018136  75.041736  \n",
       "2011-01-02     0.050  98.360656     0.018214  74.831650  \n",
       "2011-01-03     0.050  98.360656     0.016429  75.402884  \n",
       "2011-01-04     0.047  90.163934     0.015286  64.583333  \n",
       "2011-01-05     0.049  96.500000     0.014000  90.397805  \n",
       "...              ...        ...          ...        ...  \n",
       "2023-11-10  2209.580  68.735235  1173.541429  77.472798  \n",
       "2023-11-11  2663.780  83.451695  1180.479286  84.113197  \n",
       "2023-11-12  1702.460  78.426595  1160.555714  80.518099  \n",
       "2023-11-13  2132.610  76.838135  1181.945000  77.550176  \n",
       "2023-11-14  1750.230  61.269876  1218.451429  69.210402  \n",
       "\n",
       "[4701 rows x 13 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les caractéristiques et exclure la dernière ligne\n",
    "features_raw = data_raw.drop(columns=['progression tomorrow', 'target', 'close', 'high', 'low', 'volumefrom']).iloc[:-1, :]\n",
    "target = data_raw['target'].iloc[:-1]\n",
    "window_size = 1500\n",
    "n_days = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_xgb = {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
    "\n",
    "xgboost = XGBClassifier(**best_params_xgb, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0008931794508802"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model_proba_metric(data_raw, xgboost, features, target, window_size, n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = {\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.5887799564270153\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5833333333333334\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.5999455337690632\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5923202614379085\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.593681917211329\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5874183006535948\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.6029411764705882\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5925925925925926\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.5980392156862745\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5912309368191722\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.6013071895424836\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5863289760348583\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.5980392156862745\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5895969498910676\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.5923202614379085\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5833333333333334\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.585511982570806\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5827886710239651\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.6029411764705882\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5906862745098039\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.5996732026143791\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5893246187363834\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.6056644880174292\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5944989106753813\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.6043028322440087\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5944989106753813\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.6002178649237473\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5874183006535948\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, accuracy : 0.5999455337690632\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, accuracy : 0.5885076252723311\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, accuracy : 0.6015795206971678\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, accuracy : 0.5841503267973857\"\n",
      "Best Model Accuracy: 60.56645%\n",
      "Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='sklearn')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "# Créer une liste de combinaisons de paramètres\n",
    "grid_list = list(ParameterGrid(param_grid_xgb))\n",
    "\n",
    "# Pour boucler sur chaque combinaison :\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "for params in grid_list:\n",
    "    try:\n",
    "        # Instancier le modèle avec les paramètres\n",
    "        model_instance = XGBClassifier(**params)\n",
    "        \n",
    "        # Appliquer la fonction train_model\n",
    "        accuracy = train_model_grid(data, model_instance, features, target, window_size)\n",
    "        \n",
    "        # Si le modèle actuel a une meilleure précision que le précédent meilleur modèle, stocker sa précision et ses paramètres\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = params\n",
    "            print(f\"Nouveaux meilleurs paramètres trouvés : {params}, accuracy : {accuracy}\")\n",
    "\n",
    "        else:\n",
    "            print(f'Trop nul la honte : {params}, accuracy : {accuracy}\"')\n",
    "\n",
    "        \n",
    "            \n",
    "    except Exception as e:\n",
    "        # Gérer les combinaisons de paramètres non compatibles\n",
    "        error_message = f\"Error with parameters {params}: {e}\"\n",
    "        \n",
    "\n",
    "print(f\"Best Model Accuracy: {best_accuracy * 100:.5f}%\")\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.0084009335278077\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.0083829500965855\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0205983479391452\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.020363818651446\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.013274166001476\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.0135417707667784\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0282829939921057\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.0289380501062246\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.0492214962034623\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.0461964906045014\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0598968818000603\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.0564741754830942\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.0554043480703776\"\n",
      "Trop nul la honte : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.0564663984056746\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0652933395745423\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.0678019107384453\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.0129949662206572\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.012821216645928\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0242204834996174\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.0232371985916304\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.0197043519555151\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.0205513204866385\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0338016611112675\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.034420523759096\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.0518600748332796\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.0475903178455197\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0626972479357284\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.0578104539399624\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 0.8}, earning : 1.063850805826207\"\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100, 'subsample': 1.0}, earning : 1.0592964781423453\"\n",
      "Nouveaux meilleurs paramètres trouvés : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}, earning : 1.0727029942679231\n",
      "Trop nul la honte : {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 1.0}, earning : 1.0677705446894292\"\n",
      "Best Earnings: 107.27030%\n",
      "Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='sklearn')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "# Créer une liste de combinaisons de paramètres\n",
    "grid_list = list(ParameterGrid(param_grid_xgb))\n",
    "\n",
    "# Pour boucler sur chaque combinaison :\n",
    "best_earning = 0\n",
    "best_params = None\n",
    "\n",
    "for params in grid_list:\n",
    "    try:\n",
    "        # Instancier le modèle avec les paramètres\n",
    "        model_instance = XGBClassifier(**params, random_state = 42)\n",
    "        \n",
    "        # Appliquer la fonction train_model\n",
    "        earning = train_model_proba_metric(data, model_instance, features, target, window_size, n_days)\n",
    "        \n",
    "        # Si le modèle actuel a une meilleure précision que le précédent meilleur modèle, stocker sa précision et ses paramètres\n",
    "        if earning > best_earning:\n",
    "            best_earning = earning\n",
    "            best_params = params\n",
    "            print(f\"Nouveaux meilleurs paramètres trouvés : {params}, earning : {earning}\")\n",
    "\n",
    "        else:\n",
    "            print(f'Trop nul la honte : {params}, earning : {earning}')\n",
    "\n",
    "        \n",
    "            \n",
    "    except Exception as e:\n",
    "        # Gérer les combinaisons de paramètres non compatibles\n",
    "        error_message = f\"Error with parameters {params}: {e}\"\n",
    "        \n",
    "\n",
    "print(f\"Best Earnings: {best_earning * 100:.5f}%\")\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
