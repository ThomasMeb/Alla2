{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns=200\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/btc_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les caractéristiques et exclure la dernière ligne\n",
    "features = data.drop(columns=['progression tomorrow', 'target', 'close', 'high', 'low', 'volumefrom', 'market_cap', 'difficulty']).iloc[:-1, :]\n",
    "target = data['target'].iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la taille de la fenêtre initiale\n",
    "window_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, model, features, target, window_size):\n",
    "      \n",
    "      # Initialiser les listes pour stocker les prédictions et les vraies valeurs\n",
    "      predictions = []\n",
    "      actuals = []\n",
    "\n",
    "      # Boucle à travers les données de la taille de la fenêtre jusqu'à la fin des données\n",
    "      for i in range(window_size, len(data) - 1):\n",
    "            # Diviser les données en ensembles d'entraînement et de test\n",
    "            X_train = features.iloc[i-window_size:i, :]\n",
    "            y_train = target.iloc[i-window_size:i]\n",
    "            X_test = features.iloc[i:i+1, :]\n",
    "            y_test = target.iloc[i]\n",
    "\n",
    "            #Normaliser les données\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # Entraîner un modèle\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Faire une prédiction\n",
    "            prediction = model.predict(X_test)[0]\n",
    "            \n",
    "            # Stocker les prédictions et les vraies valeurs\n",
    "            predictions.append(prediction)\n",
    "            actuals.append(y_test)\n",
    "\n",
    "      # Évaluer le modèle\n",
    "      accuracy = accuracy_score(actuals, predictions)\n",
    "      print(f'Model Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 52.23%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_model = DummyClassifier(strategy=\"most_frequent\")\n",
    "train_model(data, dummy_model, features, target, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names):\n",
    "    # Récupérer les coefficients du modèle\n",
    "    coefficients = model.coef_[0]\n",
    "    \n",
    "    # Créer un DataFrame pour stocker les caractéristiques et leurs importances\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': coefficients\n",
    "    })\n",
    "    \n",
    "    # Trier le DataFrame en fonction de l'importance\n",
    "    feature_importance = feature_importance.sort_values(by='Importance', key=abs, ascending=False)\n",
    "    \n",
    "    # Visualiser l'importance des caractéristiques\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance for Logistic Regression')\n",
    "    plt.gca().invert_yaxis()  # Pour afficher la caractéristique la plus importante en haut\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volumefrom</th>\n",
       "      <th>volumeto</th>\n",
       "      <th>close</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>progression daily</th>\n",
       "      <th>progression tomorrow</th>\n",
       "      <th>target</th>\n",
       "      <th>ema_26</th>\n",
       "      <th>ema_12</th>\n",
       "      <th>macd</th>\n",
       "      <th>rsi</th>\n",
       "      <th>relative_volume</th>\n",
       "      <th>obv</th>\n",
       "      <th>atr</th>\n",
       "      <th>bollinger_upper</th>\n",
       "      <th>bollinger_lower</th>\n",
       "      <th>k</th>\n",
       "      <th>momentum</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-01</th>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>2821.24</td>\n",
       "      <td>8.419500e+02</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.251628</td>\n",
       "      <td>0.272215</td>\n",
       "      <td>0.020587</td>\n",
       "      <td>75.041736</td>\n",
       "      <td>0.433468</td>\n",
       "      <td>5.683560e+03</td>\n",
       "      <td>0.018136</td>\n",
       "      <td>0.302597</td>\n",
       "      <td>0.209763</td>\n",
       "      <td>98.360656</td>\n",
       "      <td>0.060</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-02</th>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>5352.11</td>\n",
       "      <td>1.584660e+03</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.255211</td>\n",
       "      <td>0.276490</td>\n",
       "      <td>0.021279</td>\n",
       "      <td>74.831650</td>\n",
       "      <td>0.784203</td>\n",
       "      <td>4.098900e+03</td>\n",
       "      <td>0.018214</td>\n",
       "      <td>0.307253</td>\n",
       "      <td>0.213107</td>\n",
       "      <td>98.360656</td>\n",
       "      <td>0.050</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-03</th>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>1425.19</td>\n",
       "      <td>4.208500e+02</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.016667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.258529</td>\n",
       "      <td>0.280107</td>\n",
       "      <td>0.021578</td>\n",
       "      <td>75.402884</td>\n",
       "      <td>0.227126</td>\n",
       "      <td>3.678050e+03</td>\n",
       "      <td>0.016429</td>\n",
       "      <td>0.311694</td>\n",
       "      <td>0.215676</td>\n",
       "      <td>98.360656</td>\n",
       "      <td>0.050</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-04</th>\n",
       "      <td>0.2999</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.2950</td>\n",
       "      <td>1879.00</td>\n",
       "      <td>5.483300e+02</td>\n",
       "      <td>0.2989</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.016667</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>1</td>\n",
       "      <td>0.261230</td>\n",
       "      <td>0.282398</td>\n",
       "      <td>0.021168</td>\n",
       "      <td>64.583333</td>\n",
       "      <td>0.317246</td>\n",
       "      <td>3.129720e+03</td>\n",
       "      <td>0.015286</td>\n",
       "      <td>0.315355</td>\n",
       "      <td>0.216845</td>\n",
       "      <td>90.163934</td>\n",
       "      <td>0.047</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-05</th>\n",
       "      <td>0.2990</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.2989</td>\n",
       "      <td>357.16</td>\n",
       "      <td>1.061900e+02</td>\n",
       "      <td>0.2990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>1</td>\n",
       "      <td>0.264021</td>\n",
       "      <td>0.284937</td>\n",
       "      <td>0.020916</td>\n",
       "      <td>90.397805</td>\n",
       "      <td>0.063656</td>\n",
       "      <td>3.235910e+03</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.318771</td>\n",
       "      <td>0.219319</td>\n",
       "      <td>96.500000</td>\n",
       "      <td>0.049</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-13</th>\n",
       "      <td>27110.9100</td>\n",
       "      <td>26674.970</td>\n",
       "      <td>26756.2300</td>\n",
       "      <td>18507.04</td>\n",
       "      <td>4.967794e+08</td>\n",
       "      <td>26862.9000</td>\n",
       "      <td>5.216077e+11</td>\n",
       "      <td>-0.004279</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>1</td>\n",
       "      <td>27129.215560</td>\n",
       "      <td>27294.402418</td>\n",
       "      <td>165.186858</td>\n",
       "      <td>46.835697</td>\n",
       "      <td>0.904417</td>\n",
       "      <td>1.270529e+10</td>\n",
       "      <td>785.304286</td>\n",
       "      <td>28397.807436</td>\n",
       "      <td>25925.062564</td>\n",
       "      <td>10.940893</td>\n",
       "      <td>-750.990</td>\n",
       "      <td>5.732151e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-14</th>\n",
       "      <td>26982.7800</td>\n",
       "      <td>26810.020</td>\n",
       "      <td>26862.9000</td>\n",
       "      <td>5865.21</td>\n",
       "      <td>1.576854e+08</td>\n",
       "      <td>26854.5200</td>\n",
       "      <td>5.236556e+11</td>\n",
       "      <td>0.003987</td>\n",
       "      <td>-0.000312</td>\n",
       "      <td>0</td>\n",
       "      <td>27109.488482</td>\n",
       "      <td>27228.017431</td>\n",
       "      <td>118.528949</td>\n",
       "      <td>49.455572</td>\n",
       "      <td>0.288510</td>\n",
       "      <td>1.286298e+10</td>\n",
       "      <td>786.607857</td>\n",
       "      <td>28390.511290</td>\n",
       "      <td>25960.107710</td>\n",
       "      <td>16.135237</td>\n",
       "      <td>-565.760</td>\n",
       "      <td>5.732151e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-15</th>\n",
       "      <td>27297.2600</td>\n",
       "      <td>26813.060</td>\n",
       "      <td>26854.5200</td>\n",
       "      <td>10978.68</td>\n",
       "      <td>2.971100e+08</td>\n",
       "      <td>27177.4100</td>\n",
       "      <td>5.239792e+11</td>\n",
       "      <td>-0.000312</td>\n",
       "      <td>0.012024</td>\n",
       "      <td>1</td>\n",
       "      <td>27090.601927</td>\n",
       "      <td>27170.556287</td>\n",
       "      <td>79.954360</td>\n",
       "      <td>48.700891</td>\n",
       "      <td>0.555745</td>\n",
       "      <td>1.256587e+10</td>\n",
       "      <td>738.972857</td>\n",
       "      <td>28353.377450</td>\n",
       "      <td>26056.549550</td>\n",
       "      <td>15.727169</td>\n",
       "      <td>-935.280</td>\n",
       "      <td>5.732151e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-16</th>\n",
       "      <td>30009.1500</td>\n",
       "      <td>27131.480</td>\n",
       "      <td>27177.4100</td>\n",
       "      <td>66720.48</td>\n",
       "      <td>1.884463e+09</td>\n",
       "      <td>28518.3700</td>\n",
       "      <td>5.296435e+11</td>\n",
       "      <td>0.012024</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>1</td>\n",
       "      <td>27097.032155</td>\n",
       "      <td>27171.610705</td>\n",
       "      <td>74.578550</td>\n",
       "      <td>38.382575</td>\n",
       "      <td>3.228924</td>\n",
       "      <td>1.445033e+10</td>\n",
       "      <td>848.527857</td>\n",
       "      <td>28315.553845</td>\n",
       "      <td>26182.321155</td>\n",
       "      <td>18.572004</td>\n",
       "      <td>-237.180</td>\n",
       "      <td>5.732151e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-10-17</th>\n",
       "      <td>28613.2400</td>\n",
       "      <td>28094.140</td>\n",
       "      <td>28518.3700</td>\n",
       "      <td>10541.85</td>\n",
       "      <td>2.990360e+08</td>\n",
       "      <td>28348.8300</td>\n",
       "      <td>5.553388e+11</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>-0.005945</td>\n",
       "      <td>0</td>\n",
       "      <td>27202.316440</td>\n",
       "      <td>27378.804442</td>\n",
       "      <td>176.488003</td>\n",
       "      <td>61.568243</td>\n",
       "      <td>0.533373</td>\n",
       "      <td>1.474937e+10</td>\n",
       "      <td>892.198571</td>\n",
       "      <td>28457.319281</td>\n",
       "      <td>26271.133719</td>\n",
       "      <td>57.131930</td>\n",
       "      <td>571.760</td>\n",
       "      <td>6.103068e+13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4673 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  high        low        open  volumefrom      volumeto  \\\n",
       "2011-01-01      0.3000      0.292      0.3000     2821.24  8.419500e+02   \n",
       "2011-01-02      0.3000      0.289      0.3000     5352.11  1.584660e+03   \n",
       "2011-01-03      0.3000      0.290      0.3000     1425.19  4.208500e+02   \n",
       "2011-01-04      0.2999      0.289      0.2950     1879.00  5.483300e+02   \n",
       "2011-01-05      0.2990      0.290      0.2989      357.16  1.061900e+02   \n",
       "...                ...        ...         ...         ...           ...   \n",
       "2023-10-13  27110.9100  26674.970  26756.2300    18507.04  4.967794e+08   \n",
       "2023-10-14  26982.7800  26810.020  26862.9000     5865.21  1.576854e+08   \n",
       "2023-10-15  27297.2600  26813.060  26854.5200    10978.68  2.971100e+08   \n",
       "2023-10-16  30009.1500  27131.480  27177.4100    66720.48  1.884463e+09   \n",
       "2023-10-17  28613.2400  28094.140  28518.3700    10541.85  2.990360e+08   \n",
       "\n",
       "                 close    market_cap  progression daily  progression tomorrow  \\\n",
       "2011-01-01      0.3000           NaN           0.000000              0.000000   \n",
       "2011-01-02      0.3000           NaN           0.000000              0.000000   \n",
       "2011-01-03      0.2950           NaN           0.000000             -0.016667   \n",
       "2011-01-04      0.2989           NaN          -0.016667              0.013220   \n",
       "2011-01-05      0.2990           NaN           0.013220              0.000335   \n",
       "...                ...           ...                ...                   ...   \n",
       "2023-10-13  26862.9000  5.216077e+11          -0.004279              0.003987   \n",
       "2023-10-14  26854.5200  5.236556e+11           0.003987             -0.000312   \n",
       "2023-10-15  27177.4100  5.239792e+11          -0.000312              0.012024   \n",
       "2023-10-16  28518.3700  5.296435e+11           0.012024              0.049341   \n",
       "2023-10-17  28348.8300  5.553388e+11           0.049341             -0.005945   \n",
       "\n",
       "            target        ema_26        ema_12        macd        rsi  \\\n",
       "2011-01-01       0      0.251628      0.272215    0.020587  75.041736   \n",
       "2011-01-02       0      0.255211      0.276490    0.021279  74.831650   \n",
       "2011-01-03       0      0.258529      0.280107    0.021578  75.402884   \n",
       "2011-01-04       1      0.261230      0.282398    0.021168  64.583333   \n",
       "2011-01-05       1      0.264021      0.284937    0.020916  90.397805   \n",
       "...            ...           ...           ...         ...        ...   \n",
       "2023-10-13       1  27129.215560  27294.402418  165.186858  46.835697   \n",
       "2023-10-14       0  27109.488482  27228.017431  118.528949  49.455572   \n",
       "2023-10-15       1  27090.601927  27170.556287   79.954360  48.700891   \n",
       "2023-10-16       1  27097.032155  27171.610705   74.578550  38.382575   \n",
       "2023-10-17       0  27202.316440  27378.804442  176.488003  61.568243   \n",
       "\n",
       "            relative_volume           obv         atr  bollinger_upper  \\\n",
       "2011-01-01         0.433468  5.683560e+03    0.018136         0.302597   \n",
       "2011-01-02         0.784203  4.098900e+03    0.018214         0.307253   \n",
       "2011-01-03         0.227126  3.678050e+03    0.016429         0.311694   \n",
       "2011-01-04         0.317246  3.129720e+03    0.015286         0.315355   \n",
       "2011-01-05         0.063656  3.235910e+03    0.014000         0.318771   \n",
       "...                     ...           ...         ...              ...   \n",
       "2023-10-13         0.904417  1.270529e+10  785.304286     28397.807436   \n",
       "2023-10-14         0.288510  1.286298e+10  786.607857     28390.511290   \n",
       "2023-10-15         0.555745  1.256587e+10  738.972857     28353.377450   \n",
       "2023-10-16         3.228924  1.445033e+10  848.527857     28315.553845   \n",
       "2023-10-17         0.533373  1.474937e+10  892.198571     28457.319281   \n",
       "\n",
       "            bollinger_lower          k  momentum    difficulty  \n",
       "2011-01-01         0.209763  98.360656     0.060           NaN  \n",
       "2011-01-02         0.213107  98.360656     0.050           NaN  \n",
       "2011-01-03         0.215676  98.360656     0.050           NaN  \n",
       "2011-01-04         0.216845  90.163934     0.047           NaN  \n",
       "2011-01-05         0.219319  96.500000     0.049           NaN  \n",
       "...                     ...        ...       ...           ...  \n",
       "2023-10-13     25925.062564  10.940893  -750.990  5.732151e+13  \n",
       "2023-10-14     25960.107710  16.135237  -565.760  5.732151e+13  \n",
       "2023-10-15     26056.549550  15.727169  -935.280  5.732151e+13  \n",
       "2023-10-16     26182.321155  18.572004  -237.180  5.732151e+13  \n",
       "2023-10-17     26271.133719  57.131930   571.760  6.103068e+13  \n",
       "\n",
       "[4673 rows x 22 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4672"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earn_metric(predicted_probs, progressions, n_days, i):\n",
    "      base = c = 1\n",
    "      for j in range(n_days):\n",
    "            index = len(predicted_probs) - n_days + j\n",
    "            c *= predicted_probs[index] * progressions[index] + (1 - predicted_probs[index])\n",
    "            base *= progressions[index]\n",
    "      return c / base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earn_metric(predicted_probs, progressions, n_days, i):\n",
    "      base = c = 1\n",
    "      for j in range(n_days):\n",
    "            index = len(predicted_probs) - n_days + j\n",
    "            c *= predicted_probs[index] * progressions[index] + (1 - predicted_probs[index])\n",
    "            base *= progressions[index]\n",
    "      return c / base\n",
    "\n",
    "\n",
    "def train_model_proba_metric(data, model, features, target, window_size, n_days):\n",
    "      \n",
    "    # Initialiser les listes pour stocker les probabilités prédites et les vraies valeurs\n",
    "    predicted_probs = []\n",
    "    progressions = []\n",
    "    metric = []\n",
    "\n",
    "    # Boucle à travers les données de la taille de la fenêtre jusqu'à la fin des données\n",
    "    for i in range(window_size, len(data) - 1):\n",
    "        # Diviser les données en ensembles d'entraînement et de test\n",
    "        X_train = features.iloc[i-window_size:i, :]\n",
    "        y_train = target.iloc[i-window_size:i]\n",
    "        X_test = features.iloc[i:i+1, :]\n",
    "        y_test = target.iloc[i]\n",
    "\n",
    "        # Normaliser les données\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Entraîner un modèle\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Obtenir les probabilités prédites pour la classe positive (par exemple, 1)\n",
    "        prediction_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # Stocker les probabilités prédites et les vraies valeurs\n",
    "        predicted_probs.extend(prediction_prob)\n",
    "        \n",
    "        # Récupérer la progression réelle\n",
    "        progressions.append(data.iloc[i]['progression tomorrow']+1)\n",
    "\n",
    "        if i >= window_size + n_days:\n",
    "            metric.append(earn_metric(predicted_probs, progressions, n_days, i))\n",
    "    \n",
    "    return metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_regression = LogisticRegression()\n",
    "n_days = 31\n",
    "earnings = train_model_proba_metric(data, logistic_regression, features, target, window_size, n_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0443524998161493"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(earnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 0\n",
      "progres 1\n",
      "c : 1.0080141881530016\n",
      "base : 1.013347921225383\n",
      "----------------------------------------\n",
      "round 1\n",
      "progres 1\n",
      "c : 1.020445935000118\n",
      "base : 1.0393873085339171\n",
      "----------------------------------------\n",
      "round 2\n",
      "progres 1\n",
      "c : 1.0258641097949759\n",
      "base : 1.0494529540481403\n",
      "----------------------------------------\n",
      "round 3\n",
      "progres 0\n",
      "c : 1.0194220140560268\n",
      "base : 1.0350109409190378\n",
      "----------------------------------------\n",
      "round 4\n",
      "progres 0\n",
      "c : 1.0143388633742798\n",
      "base : 1.0233406272793588\n",
      "----------------------------------------\n",
      "round 5\n",
      "progres 0\n",
      "c : 0.9625493866913664\n",
      "base : 0.8971553610503288\n",
      "----------------------------------------\n",
      "round 6\n",
      "progres 1\n",
      "c : 0.9898226805290491\n",
      "base : 0.9554339897884762\n",
      "----------------------------------------\n",
      "round 7\n",
      "progres 1\n",
      "c : 1.0089689614768238\n",
      "base : 0.9979576951130567\n",
      "----------------------------------------\n",
      "round 8\n",
      "progres 0\n",
      "c : 1.0086249159379632\n",
      "base : 0.9970824215900808\n",
      "----------------------------------------\n",
      "round 9\n",
      "progres 1\n",
      "c : 1.0116574599572303\n",
      "base : 1.0051057622173603\n",
      "----------------------------------------\n",
      "round 10\n",
      "progres 0\n",
      "c : 1.0059722028394027\n",
      "base : 0.9905178701677614\n",
      "----------------------------------------\n",
      "round 11\n",
      "progres 1\n",
      "c : 1.0080450413761155\n",
      "base : 0.995550692924873\n",
      "----------------------------------------\n",
      "round 12\n",
      "progres 1\n",
      "c : 1.0175109189539804\n",
      "base : 1.0175054704595192\n",
      "----------------------------------------\n",
      "round 13\n",
      "progres 1\n",
      "c : 1.020136699597112\n",
      "base : 1.0241429613420867\n",
      "----------------------------------------\n",
      "round 14\n",
      "progres 0\n",
      "c : 1.019291456308013\n",
      "base : 1.0218818380743988\n",
      "----------------------------------------\n",
      "round 15\n",
      "progres 1\n",
      "c : 1.0264085361626865\n",
      "base : 1.042231947483589\n",
      "----------------------------------------\n",
      "round 16\n",
      "progres 1\n",
      "c : 1.036166877790856\n",
      "base : 1.0760758570386584\n",
      "----------------------------------------\n",
      "round 17\n",
      "progres 1\n",
      "c : 1.0461857994941246\n",
      "base : 1.1040846097738881\n",
      "----------------------------------------\n",
      "round 18\n",
      "progres 1\n",
      "c : 1.0652555543635438\n",
      "base : 1.15309992706054\n",
      "----------------------------------------\n",
      "round 19\n",
      "progres 0\n",
      "c : 1.046882923886434\n",
      "base : 1.1146608315098472\n",
      "----------------------------------------\n",
      "round 20\n",
      "progres 1\n",
      "c : 1.069222057912787\n",
      "base : 1.1494529540481404\n",
      "----------------------------------------\n",
      "round 21\n",
      "progres 1\n",
      "c : 1.1202874185815934\n",
      "base : 1.2274252370532464\n",
      "----------------------------------------\n",
      "round 22\n",
      "progres 1\n",
      "c : 1.198615711454061\n",
      "base : 1.3358862144420136\n",
      "----------------------------------------\n",
      "round 23\n",
      "progres 1\n",
      "c : 1.2113203206818937\n",
      "base : 1.3574033552151719\n",
      "----------------------------------------\n",
      "round 24\n",
      "progres 1\n",
      "c : 1.2395957211239388\n",
      "base : 1.4061269146608322\n",
      "----------------------------------------\n",
      "round 25\n",
      "progres 1\n",
      "c : 1.2837815131681625\n",
      "base : 1.4805981035740345\n",
      "----------------------------------------\n",
      "round 26\n",
      "progres 1\n",
      "c : 1.399558387886787\n",
      "base : 1.6628008752735242\n",
      "----------------------------------------\n",
      "round 27\n",
      "progres 0\n",
      "c : 1.3233829312858065\n",
      "base : 1.5097009482129844\n",
      "----------------------------------------\n",
      "round 28\n",
      "progres 0\n",
      "c : 1.2901378638109113\n",
      "base : 1.4433260393873097\n",
      "----------------------------------------\n",
      "round 29\n",
      "progres 0\n",
      "c : 1.2543523537922656\n",
      "base : 1.3753464624361793\n",
      "----------------------------------------\n",
      "round 30\n",
      "progres 1\n",
      "c : 1.3233886546544138\n",
      "base : 1.5091903719912487\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "# Initialiser les listes pour stocker les probabilités prédites et les vraies valeurs\n",
    "predicted_probs = []\n",
    "actuals = []\n",
    "progressions = []\n",
    "\n",
    "c = 1\n",
    "base = 1\n",
    "\n",
    "for i in range(31):\n",
    "      # Diviser les données en ensembles d'entraînement et de test\n",
    "      X_train = features.iloc[i:window_size+i, :]\n",
    "      y_train = target.iloc[i:window_size+i]\n",
    "      X_test = features.iloc[window_size+i : window_size+i+1, :]\n",
    "      y_test = target.iloc[window_size+i]\n",
    "\n",
    "      # Normaliser les données\n",
    "      scaler = StandardScaler()\n",
    "      X_train_scaled = scaler.fit_transform(X_train)\n",
    "      X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "      # Entraîner un modèle\n",
    "      model.fit(X_train_scaled, y_train)\n",
    "\n",
    "      # Obtenir les probabilités prédites pour la classe positive (par exemple, 1)\n",
    "      prediction_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "      # Stocker les probabilités prédites et les vraies valeurs\n",
    "      predicted_probs.extend(prediction_prob)\n",
    "      actuals.append(y_test)\n",
    "      progressions.append(data.iloc[window_size+i]['progression tomorrow']+1)\n",
    "\n",
    "      c *= predicted_probs[i] * progressions[i] + (1 - predicted_probs[i])\n",
    "      base *= progressions[i]\n",
    "\n",
    "\n",
    "      print(f'round {i}')\n",
    "      print(f'progres {y_test}')\n",
    "      print(f'c : {c}')\n",
    "      print(f'base : {base}')\n",
    "      print('--------'*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.013347921225383,\n",
       " 1.0256963938674153,\n",
       " 1.0096842105263157,\n",
       " 0.9862385321100919,\n",
       " 0.988724453840733,\n",
       " 0.8766928011404134,\n",
       " 1.064959349593496,\n",
       " 1.044507214291167,\n",
       " 0.9991229352433855,\n",
       " 1.0080468178493052,\n",
       " 0.9854862119013063,\n",
       " 1.005081001472754,\n",
       " 1.0220528976481793,\n",
       " 1.0065232974910394,\n",
       " 0.9977921800441564,\n",
       " 1.0199143468950749,\n",
       " 1.032472531317797,\n",
       " 1.0260286043516573,\n",
       " 1.0443945299597013,\n",
       " 0.9666645581630717,\n",
       " 1.0312131919905771,\n",
       " 1.0678342534424774,\n",
       " 1.0883646303779415,\n",
       " 1.016107016107016,\n",
       " 1.0358946802794198,\n",
       " 1.0529619255109453,\n",
       " 1.1230602492733632,\n",
       " 0.9079264815545905,\n",
       " 0.956034399458885,\n",
       " 0.9529007479280373,\n",
       " 1.0973165040305473]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.60040721080683,\n",
       " 0.479947056414951,\n",
       " 0.548275436829798,\n",
       " 0.4563232295849949,\n",
       " 0.4422230635316302,\n",
       " 0.4140664330528999,\n",
       " 0.4361871849836109,\n",
       " 0.43460689675727876,\n",
       " 0.3887822819149118,\n",
       " 0.37363990309366896,\n",
       " 0.387200441772941,\n",
       " 0.4055367094480023,\n",
       " 0.4258094386710644,\n",
       " 0.39559626208842863,\n",
       " 0.37528370294677327,\n",
       " 0.3506205662074579,\n",
       " 0.2927787947594506,\n",
       " 0.3714842412223311,\n",
       " 0.4105885437975299,\n",
       " 0.5173820022615735,\n",
       " 0.6836440433816976,\n",
       " 0.704059632038982,\n",
       " 0.7912445607844866,\n",
       " 0.6580611503646457,\n",
       " 0.6503088615065241,\n",
       " 0.6730367978693366,\n",
       " 0.7328463246790808,\n",
       " 0.591138581770577,\n",
       " 0.5713847961102692,\n",
       " 0.5889210519426863,\n",
       " 0.5655505962196752]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "c *= (progressions[0]*predicted_probs[0] + 1 - predicted_probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.80141881530018"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c *= (progressions[1]*predicted_probs[1] + 1 - predicted_probs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.064959349593496"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "progressions[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "c *= predicted_probs[5] * progressions[5] + (1 - predicted_probs[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.89426279984663"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la grille de paramètres\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'max_iter': [50, 100, 200, 500],\n",
    "    'fit_intercept': [True, False],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_combinations = (\n",
    "    len(param_grid['penalty']) *\n",
    "    len(param_grid['C']) *\n",
    "    len(param_grid['solver']) *\n",
    "    len(param_grid['max_iter']) *\n",
    "    len(param_grid['fit_intercept']) *\n",
    "    len(param_grid['class_weight']) *\n",
    "    len(param_grid['l1_ratio'])\n",
    ")\n",
    "\n",
    "incompatible_l1 = 3 * len(param_grid['C']) * len(param_grid['max_iter']) * len(param_grid['fit_intercept']) * len(param_grid['class_weight']) * len(param_grid['l1_ratio'])\n",
    "incompatible_elasticnet = 4 * len(param_grid['C']) * len(param_grid['max_iter']) * len(param_grid['fit_intercept']) * len(param_grid['class_weight']) * len(param_grid['l1_ratio'])\n",
    "incompatible_none = len(param_grid['C']) * len(param_grid['max_iter']) * len(param_grid['fit_intercept']) * len(param_grid['class_weight']) * len(param_grid['l1_ratio'])\n",
    "incompatible_none = len(param_grid['C']) * len(param_grid['max_iter']) * len(param_grid['fit_intercept']) * len(param_grid['class_weight']) * len(param_grid['l1_ratio'])\n",
    "incompatible_l1_ratio = 3 * len(param_grid['C']) * len(param_grid['solver']) * len(param_grid['max_iter']) * len(param_grid['fit_intercept']) * len(param_grid['class_weight'])\n",
    "total_incompatibilities = incompatible_l1 + incompatible_elasticnet + incompatible_none + incompatible_l1_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9600"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5280"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_incompatibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4320"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_combinations - total_incompatibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_grid(data, model, features, target, window_size):\n",
    "      \n",
    "      # Initialiser les listes pour stocker les prédictions et les vraies valeurs\n",
    "      predictions = []\n",
    "      actuals = []\n",
    "\n",
    "      # Boucle à travers les données de la taille de la fenêtre jusqu'à la fin des données\n",
    "      for i in range(window_size, len(data) - 1):\n",
    "            # Diviser les données en ensembles d'entraînement et de test\n",
    "            X_train = features.iloc[i-window_size:i, :]\n",
    "            y_train = target.iloc[i-window_size:i]\n",
    "            X_test = features.iloc[i:i+1, :]\n",
    "            y_test = target.iloc[i]\n",
    "\n",
    "            #Normaliser les données\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            \n",
    "            # Entraîner un modèle\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Faire une prédiction\n",
    "            prediction = model.predict(X_test)[0]\n",
    "            \n",
    "            # Stocker les prédictions et les vraies valeurs\n",
    "            predictions.append(prediction)\n",
    "            actuals.append(y_test)\n",
    "\n",
    "      # Évaluer le modèle\n",
    "      accuracy = accuracy_score(actuals, predictions)\n",
    "      return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<small>Error with parameters {'C': 0.001, 'class_weight': None, 'fit_intercept': True, 'l1_ratio': 0.1, 'max_iter': 50, 'penalty': 'l1', 'solver': 'newton-cg'}: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small>Error with parameters {'C': 0.001, 'class_weight': None, 'fit_intercept': True, 'l1_ratio': 0.1, 'max_iter': 50, 'penalty': 'l1', 'solver': 'lbfgs'}: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouveaux meilleurs paramètres trouvés : {'C': 0.001, 'class_weight': None, 'fit_intercept': True, 'l1_ratio': 0.1, 'max_iter': 50, 'penalty': 'l1', 'solver': 'liblinear'}, accuracy : 0.47766884531590414\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<small>Error with parameters {'C': 0.001, 'class_weight': None, 'fit_intercept': True, 'l1_ratio': 0.1, 'max_iter': 50, 'penalty': 'l1', 'solver': 'sag'}: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.</small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thoma\\Desktop\\Code\\tradebtcai\\poubelle.ipynb Cellule 39\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/Code/tradebtcai/poubelle.ipynb#X56sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model_instance \u001b[39m=\u001b[39m LogisticRegression(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/Code/tradebtcai/poubelle.ipynb#X56sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Appliquer la fonction train_model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/Code/tradebtcai/poubelle.ipynb#X56sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m accuracy \u001b[39m=\u001b[39m train_model_grid(data, model_instance, features, target, window_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/Code/tradebtcai/poubelle.ipynb#X56sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Si le modèle actuel a une meilleure précision que le précédent meilleur modèle, stocker sa précision et ses paramètres\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/Code/tradebtcai/poubelle.ipynb#X56sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m accuracy \u001b[39m>\u001b[39m best_accuracy:\n",
      "\u001b[1;32mc:\\Users\\thoma\\Desktop\\Code\\tradebtcai\\poubelle.ipynb Cellule 39\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/Code/tradebtcai/poubelle.ipynb#X56sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m X_test \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mtransform(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/Code/tradebtcai/poubelle.ipynb#X56sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Entraîner un modèle\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/Code/tradebtcai/poubelle.ipynb#X56sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/Code/tradebtcai/poubelle.ipynb#X56sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Faire une prédiction\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/Code/tradebtcai/poubelle.ipynb#X56sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m prediction \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1303\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1301\u001b[0m     n_threads \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1303\u001b[0m fold_coefs_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, prefer\u001b[39m=\u001b[39;49mprefer)(\n\u001b[0;32m   1304\u001b[0m     path_func(\n\u001b[0;32m   1305\u001b[0m         X,\n\u001b[0;32m   1306\u001b[0m         y,\n\u001b[0;32m   1307\u001b[0m         pos_class\u001b[39m=\u001b[39;49mclass_,\n\u001b[0;32m   1308\u001b[0m         Cs\u001b[39m=\u001b[39;49m[C_],\n\u001b[0;32m   1309\u001b[0m         l1_ratio\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[0;32m   1310\u001b[0m         fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[0;32m   1311\u001b[0m         tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m   1312\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m   1313\u001b[0m         solver\u001b[39m=\u001b[39;49msolver,\n\u001b[0;32m   1314\u001b[0m         multi_class\u001b[39m=\u001b[39;49mmulti_class,\n\u001b[0;32m   1315\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m   1316\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m   1317\u001b[0m         check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1318\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[0;32m   1319\u001b[0m         coef\u001b[39m=\u001b[39;49mwarm_start_coef_,\n\u001b[0;32m   1320\u001b[0m         penalty\u001b[39m=\u001b[39;49mpenalty,\n\u001b[0;32m   1321\u001b[0m         max_squared_sum\u001b[39m=\u001b[39;49mmax_squared_sum,\n\u001b[0;32m   1322\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1323\u001b[0m         n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[0;32m   1324\u001b[0m     )\n\u001b[0;32m   1325\u001b[0m     \u001b[39mfor\u001b[39;49;00m class_, warm_start_coef_ \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(classes_, warm_start_coef)\n\u001b[0;32m   1326\u001b[0m )\n\u001b[0;32m   1328\u001b[0m fold_coefs_, _, n_iter_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(n_iter_, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:533\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    530\u001b[0m         alpha \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m l1_ratio)\n\u001b[0;32m    531\u001b[0m         beta \u001b[39m=\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C) \u001b[39m*\u001b[39m l1_ratio\n\u001b[1;32m--> 533\u001b[0m     w0, n_iter_i, warm_start_sag \u001b[39m=\u001b[39m sag_solver(\n\u001b[0;32m    534\u001b[0m         X,\n\u001b[0;32m    535\u001b[0m         target,\n\u001b[0;32m    536\u001b[0m         sample_weight,\n\u001b[0;32m    537\u001b[0m         loss,\n\u001b[0;32m    538\u001b[0m         alpha,\n\u001b[0;32m    539\u001b[0m         beta,\n\u001b[0;32m    540\u001b[0m         max_iter,\n\u001b[0;32m    541\u001b[0m         tol,\n\u001b[0;32m    542\u001b[0m         verbose,\n\u001b[0;32m    543\u001b[0m         random_state,\n\u001b[0;32m    544\u001b[0m         \u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    545\u001b[0m         max_squared_sum,\n\u001b[0;32m    546\u001b[0m         warm_start_sag,\n\u001b[0;32m    547\u001b[0m         is_saga\u001b[39m=\u001b[39;49m(solver \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    548\u001b[0m     )\n\u001b[0;32m    550\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    551\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    552\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msolver must be one of \u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlbfgs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    553\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnewton-cg\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39msag\u001b[39m\u001b[39m'\u001b[39m\u001b[39m}, got \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m solver\n\u001b[0;32m    554\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\thoma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:325\u001b[0m, in \u001b[0;36msag_solver\u001b[1;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mZeroDivisionError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCurrent sag implementation does not handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe case step_size * alpha_scaled == 1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[0;32m    324\u001b[0m sag \u001b[39m=\u001b[39m sag64 \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mfloat64 \u001b[39melse\u001b[39;00m sag32\n\u001b[1;32m--> 325\u001b[0m num_seen, n_iter_ \u001b[39m=\u001b[39m sag(\n\u001b[0;32m    326\u001b[0m     dataset,\n\u001b[0;32m    327\u001b[0m     coef_init,\n\u001b[0;32m    328\u001b[0m     intercept_init,\n\u001b[0;32m    329\u001b[0m     n_samples,\n\u001b[0;32m    330\u001b[0m     n_features,\n\u001b[0;32m    331\u001b[0m     n_classes,\n\u001b[0;32m    332\u001b[0m     tol,\n\u001b[0;32m    333\u001b[0m     max_iter,\n\u001b[0;32m    334\u001b[0m     loss,\n\u001b[0;32m    335\u001b[0m     step_size,\n\u001b[0;32m    336\u001b[0m     alpha_scaled,\n\u001b[0;32m    337\u001b[0m     beta_scaled,\n\u001b[0;32m    338\u001b[0m     sum_gradient_init,\n\u001b[0;32m    339\u001b[0m     gradient_memory_init,\n\u001b[0;32m    340\u001b[0m     seen_init,\n\u001b[0;32m    341\u001b[0m     num_seen_init,\n\u001b[0;32m    342\u001b[0m     fit_intercept,\n\u001b[0;32m    343\u001b[0m     intercept_sum_gradient,\n\u001b[0;32m    344\u001b[0m     intercept_decay,\n\u001b[0;32m    345\u001b[0m     is_saga,\n\u001b[0;32m    346\u001b[0m     verbose,\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    349\u001b[0m \u001b[39mif\u001b[39;00m n_iter_ \u001b[39m==\u001b[39m max_iter:\n\u001b[0;32m    350\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    351\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe max_iter was reached which means the coef_ did not converge\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    352\u001b[0m         ConvergenceWarning,\n\u001b[0;32m    353\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='sklearn')\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "# Créer une liste de combinaisons de paramètres\n",
    "grid_list = list(ParameterGrid(param_grid))\n",
    "\n",
    "# Pour boucler sur chaque combinaison :\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "\n",
    "for params in grid_list:\n",
    "    try:\n",
    "        # Instancier le modèle avec les paramètres\n",
    "        model_instance = LogisticRegression(**params)\n",
    "        \n",
    "        # Appliquer la fonction train_model\n",
    "        accuracy = train_model_grid(data, model_instance, features, target, window_size)\n",
    "        \n",
    "        # Si le modèle actuel a une meilleure précision que le précédent meilleur modèle, stocker sa précision et ses paramètres\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = params\n",
    "            print(f\"Nouveaux meilleurs paramètres trouvés : {params}, accuracy : {accuracy}\")\n",
    "\n",
    "        \n",
    "            \n",
    "    except Exception as e:\n",
    "        # Gérer les combinaisons de paramètres non compatibles\n",
    "        error_message = f\"Error with parameters {params}: {e}\"\n",
    "        display(HTML(f\"<small>{error_message}</small>\"))\n",
    "        #pass\n",
    "\n",
    "print(f\"Best Model Accuracy: {best_accuracy * 100:.2f}%\")\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.001,\n",
       " 'class_weight': None,\n",
       " 'fit_intercept': True,\n",
       " 'l1_ratio': 0.1,\n",
       " 'max_iter': 50,\n",
       " 'penalty': 'l1',\n",
       " 'solver': 'liblinear'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
