{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns=200\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/btc_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crop = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les caractéristiques et exclure la dernière ligne\n",
    "features = data_crop.drop(columns=['progression tomorrow', 'target', 'close', 'high', 'low', 'volumefrom']).iloc[:-1, :]\n",
    "target = data['target'].iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3825"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 1. Préparation des données\\n\\nX = features.values\\ny = target.values\\n\\nwindow_size = 1000 # Vous pouvez ajuster cette valeur selon vos besoins\\n\\n# 2. Construction du modèle\\n\\nmodel = keras.Sequential([\\n    layers.InputLayer(input_shape=(X.shape[1],)),\\n    layers.Dense(64, activation=\\'relu\\'),\\n    layers.Dense(32, activation=\\'relu\\'),\\n    layers.Dense(1, activation=\\'sigmoid\\')\\n])\\n\\nmodel.compile(optimizer=\\'adam\\', loss=\\'binary_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# 3. Entraînement du modèle avec validation walk-forward\\n\\nfor i in range(window_size, len(X) - 1):\\n    # Séparation des données en ensembles d\\'entraînement et de test\\n    X_train = X[i-window_size:i]\\n    y_train = y[i-window_size:i]\\n    X_test = X[i:i+1]\\n    y_test = y[i:i+1]\\n\\n    # Normalisation des caractéristiques\\n    scaler = StandardScaler()\\n    X_train = scaler.fit_transform(X_train)\\n    X_test = scaler.transform(X_test)\\n\\n    # Entraînement du modèle\\n    model.fit(X_train, y_train, epochs=10, verbose=0)  # Vous pouvez ajuster le nombre d\\'epochs selon vos besoins\\n\\n    # Évaluation du modèle (optionnel)\\n    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\\n    print(f\"Step {i}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# 1. Préparation des données\n",
    "\n",
    "X = features.values\n",
    "y = target.values\n",
    "\n",
    "window_size = 1000 # Vous pouvez ajuster cette valeur selon vos besoins\n",
    "\n",
    "# 2. Construction du modèle\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(X.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. Entraînement du modèle avec validation walk-forward\n",
    "\n",
    "for i in range(window_size, len(X) - 1):\n",
    "    # Séparation des données en ensembles d'entraînement et de test\n",
    "    X_train = X[i-window_size:i]\n",
    "    y_train = y[i-window_size:i]\n",
    "    X_test = X[i:i+1]\n",
    "    y_test = y[i:i+1]\n",
    "\n",
    "    # Normalisation des caractéristiques\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    model.fit(X_train, y_train, epochs=10, verbose=0)  # Vous pouvez ajuster le nombre d'epochs selon vos besoins\n",
    "\n",
    "    # Évaluation du modèle (optionnel)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Step {i}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Définir la métrique personnalisée\n",
    "\n",
    "def earn_metric(predicted_probs, progressions, n_days):\n",
    "    base = c = 1\n",
    "    for j in range(n_days):\n",
    "        index = len(predicted_probs) - n_days + j\n",
    "        c *= predicted_probs[index] * progressions[index] + (1 - predicted_probs[index])\n",
    "        base *= progressions[index]\n",
    "    return c / base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Préparation des données\\n\\nX = features.values\\ny = target.values\\n\\nwindow_size = 2000  \\nn_days = 30 \\n\\n# Construction du modèle\\n\\nmodel = keras.Sequential([\\n    layers.InputLayer(input_shape=(X.shape[1],)),\\n    layers.Dense(64, activation=\\'relu\\'),\\n    layers.Dense(32, activation=\\'relu\\'),\\n    layers.Dense(1, activation=\\'sigmoid\\')\\n])\\n\\nmodel.compile(optimizer=\\'adam\\', loss=\\'binary_crossentropy\\')\\n\\n# Entraînement du modèle avec validation walk-forward\\n\\npredicted_probs = []\\nprogressions = []\\nmetrics = []\\n\\nfor i in range(window_size, len(X) - 1 - n_days):\\n    # Séparation des données\\n    X_train = X[i-window_size:i]\\n    y_train = y[i-window_size:i]\\n    X_test = X[i:i+n_days]\\n    y_test = y[i:i+n_days]\\n\\n    # Normalisation\\n    scaler = StandardScaler()\\n    X_train = scaler.fit_transform(X_train)\\n    X_test = scaler.transform(X_test)\\n\\n    with HiddenPrints():\\n        # Entraînement\\n        model.fit(X_train, y_train, epochs=20, verbose=0)\\n\\n        # Prédiction\\n        predicted_prob = model.predict(X_test).flatten()\\n        predicted_probs.extend(predicted_prob)\\n\\n    # Récupérer la progression réelle\\n    progression = data_crop.iloc[i:i+n_days][\\'progression tomorrow\\'].values + 1\\n    progressions.extend(progression)\\n\\n    # Calcul de la métrique personnalisée\\n    if len(predicted_probs) >= n_days:\\n        metric_value = earn_metric(predicted_probs, progressions, n_days)\\n        metrics.append(metric_value)\\n        #print(f\"Step {i}, Earn Metric: {metric_value:.4f}\")\\n\\nprint(f\"Average Earn Metric: {np.mean(metrics):.4f}\")'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Préparation des données\n",
    "\n",
    "X = features.values\n",
    "y = target.values\n",
    "\n",
    "window_size = 2000  \n",
    "n_days = 30 \n",
    "\n",
    "# Construction du modèle\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(X.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Entraînement du modèle avec validation walk-forward\n",
    "\n",
    "predicted_probs = []\n",
    "progressions = []\n",
    "metrics = []\n",
    "\n",
    "for i in range(window_size, len(X) - 1 - n_days):\n",
    "    # Séparation des données\n",
    "    X_train = X[i-window_size:i]\n",
    "    y_train = y[i-window_size:i]\n",
    "    X_test = X[i:i+n_days]\n",
    "    y_test = y[i:i+n_days]\n",
    "\n",
    "    # Normalisation\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    with HiddenPrints():\n",
    "        # Entraînement\n",
    "        model.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "\n",
    "        # Prédiction\n",
    "        predicted_prob = model.predict(X_test).flatten()\n",
    "        predicted_probs.extend(predicted_prob)\n",
    "\n",
    "    # Récupérer la progression réelle\n",
    "    progression = data_crop.iloc[i:i+n_days]['progression tomorrow'].values + 1\n",
    "    progressions.extend(progression)\n",
    "\n",
    "    # Calcul de la métrique personnalisée\n",
    "    if len(predicted_probs) >= n_days:\n",
    "        metric_value = earn_metric(predicted_probs, progressions, n_days)\n",
    "        metrics.append(metric_value)\n",
    "        #print(f\"Step {i}, Earn Metric: {metric_value:.4f}\")\n",
    "\n",
    "print(f\"Average Earn Metric: {np.mean(metrics):.4f}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Earn Metric: 0.9799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Préparation des données\\n\\nX = features.values\\ny = target.values\\n\\nwindow_size = 3000  \\nn_days = 30 \\n\\n# Construction du modèle LSTM\\n\\nmodel = keras.Sequential([\\n    layers.InputLayer(input_shape=(X.shape[1], 1)),  # Les LSTM attendent une entrée tridimensionnelle (batch_size, timesteps, features)\\n    layers.LSTM(64, return_sequences=True),  # Retourne une séquence à la couche suivante\\n    layers.LSTM(32),  # Retourne la dernière sortie de la séquence\\n    layers.Dense(1, activation=\\'sigmoid\\')\\n])\\n\\nmodel.compile(optimizer=\\'adam\\', loss=\\'binary_crossentropy\\')\\n\\n# Entraînement du modèle avec validation walk-forward\\n\\npredicted_probs = []\\nprogressions = []\\nmetrics = []\\n\\nfor i in range(window_size, len(X) - 1 - n_days):\\n    # Séparation des données\\n    X_train = X[i-window_size:i]\\n    y_train = y[i-window_size:i]\\n    X_test = X[i:i+n_days]\\n    y_test = y[i:i+n_days]\\n\\n    # Normalisation\\n    scaler = StandardScaler()\\n    X_train = scaler.fit_transform(X_train)\\n    X_test = scaler.transform(X_test)\\n\\n    # Les LSTM attendent une entrée tridimensionnelle, donc nous devons remodeler les données\\n    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\\n    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\\n\\n    with HiddenPrints():\\n        # Entraînement\\n        model.fit(X_train, y_train, epochs=10, verbose=0)\\n\\n        # Prédiction\\n        predicted_prob = model.predict(X_test).flatten()\\n        predicted_probs.extend(predicted_prob)\\n\\n    # Récupérer la progression réelle\\n    progression = data_crop.iloc[i:i+n_days][\\'progression tomorrow\\'].values + 1\\n    progressions.extend(progression)\\n\\n    # Calcul de la métrique personnalisée\\n    if len(predicted_probs) >= n_days:\\n        metric_value = earn_metric(predicted_probs, progressions, n_days)\\n        metrics.append(metric_value)\\n\\nprint(f\"Average Earn Metric: {np.mean(metrics):.4f}\")'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Préparation des données\n",
    "\n",
    "X = features.values\n",
    "y = target.values\n",
    "\n",
    "window_size = 3000  \n",
    "n_days = 30 \n",
    "\n",
    "# Construction du modèle LSTM\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(X.shape[1], 1)),  # Les LSTM attendent une entrée tridimensionnelle (batch_size, timesteps, features)\n",
    "    layers.LSTM(64, return_sequences=True),  # Retourne une séquence à la couche suivante\n",
    "    layers.LSTM(32),  # Retourne la dernière sortie de la séquence\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Entraînement du modèle avec validation walk-forward\n",
    "\n",
    "predicted_probs = []\n",
    "progressions = []\n",
    "metrics = []\n",
    "\n",
    "for i in range(window_size, len(X) - 1 - n_days):\n",
    "    # Séparation des données\n",
    "    X_train = X[i-window_size:i]\n",
    "    y_train = y[i-window_size:i]\n",
    "    X_test = X[i:i+n_days]\n",
    "    y_test = y[i:i+n_days]\n",
    "\n",
    "    # Normalisation\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Les LSTM attendent une entrée tridimensionnelle, donc nous devons remodeler les données\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    with HiddenPrints():\n",
    "        # Entraînement\n",
    "        model.fit(X_train, y_train, epochs=10, verbose=0)\n",
    "\n",
    "        # Prédiction\n",
    "        predicted_prob = model.predict(X_test).flatten()\n",
    "        predicted_probs.extend(predicted_prob)\n",
    "\n",
    "    # Récupérer la progression réelle\n",
    "    progression = data_crop.iloc[i:i+n_days]['progression tomorrow'].values + 1\n",
    "    progressions.extend(progression)\n",
    "\n",
    "    # Calcul de la métrique personnalisée\n",
    "    if len(predicted_probs) >= n_days:\n",
    "        metric_value = earn_metric(predicted_probs, progressions, n_days)\n",
    "        metrics.append(metric_value)\n",
    "\n",
    "print(f\"Average Earn Metric: {np.mean(metrics):.4f}\")\"\"\"\n",
    "\n",
    "#2h15 pour un average metric à 1.0055... (epoch 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras.wrappers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thoma\\Desktop\\Code\\tradebtcai\\tests_neural_network.ipynb Cellule 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/Code/tradebtcai/tests_neural_network.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwrappers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mscikit_learn\u001b[39;00m \u001b[39mimport\u001b[39;00m KerasClassifier\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/Code/tradebtcai/tests_neural_network.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m RandomizedSearchCV\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/thoma/Desktop/Code/tradebtcai/tests_neural_network.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptimizers\u001b[39;00m \u001b[39mimport\u001b[39;00m Adam\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras.wrappers'"
     ]
    }
   ],
   "source": [
    "\"\"\"from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def create_model(learning_rate=0.001, dropout_rate=0.0, lstm_units=64):\n",
    "    model = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=(X.shape[1], 1)),\n",
    "        layers.LSTM(lstm_units, return_sequences=True, dropout=dropout_rate),\n",
    "        layers.LSTM(int(lstm_units/2), dropout=dropout_rate),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=osptimizer, loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "param_dist = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'dropout_rate': [0.0, 0.1, 0.2, 0.3],\n",
    "    'lstm_units': [32, 64, 128]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=3, verbose=2, n_jobs=-1)\n",
    "random_search_result = random_search.fit(X, y)\n",
    "\n",
    "print(\"Meilleur: %f avec %s\" % (random_search_result.best_score_, random_search_result.best_params_))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best metric: 1.0393\n",
      "Best hyperparameters: {'learning_rate': 0.001, 'lstm_units': 32}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Préparation des données\n",
    "X = features.values\n",
    "y = target.values\n",
    "\n",
    "window_size = 3000  \n",
    "n_days = 30 \n",
    "\n",
    "best_metric = 0\n",
    "best_params = None\n",
    "\n",
    "# Espace de recherche\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "lstm_units = [32, 64, 128]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for units in lstm_units:\n",
    "        # Construction du modèle\n",
    "        model = keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(X.shape[1], 1)),\n",
    "            layers.LSTM(units, return_sequences=True),\n",
    "            layers.LSTM(units),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "        \n",
    "        # Entraînement du modèle avec validation walk-forward\n",
    "        predicted_probs = []\n",
    "        progressions = []\n",
    "        metrics = []\n",
    "\n",
    "        for i in range(window_size, len(X) - 1 - n_days):\n",
    "            # Séparation des données\n",
    "            X_train = X[i-window_size:i]\n",
    "            y_train = y[i-window_size:i]\n",
    "            X_test = X[i:i+n_days]\n",
    "            y_test = y[i:i+n_days]\n",
    "\n",
    "            # Normalisation\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            # Les LSTM attendent une entrée tridimensionnelle\n",
    "            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "\n",
    "            with HiddenPrints():\n",
    "                # Entraînement\n",
    "                model.fit(X_train, y_train, epochs=10, verbose=0)\n",
    "\n",
    "                # Prédiction\n",
    "                predicted_prob = model.predict(X_test).flatten()\n",
    "                predicted_probs.extend(predicted_prob)\n",
    "\n",
    "            # Récupérer la progression réelle\n",
    "            progression = data_crop.iloc[i:i+n_days]['progression tomorrow'].values + 1\n",
    "            progressions.extend(progression)\n",
    "\n",
    "            # Calcul de la métrique personnalisée\n",
    "            if len(predicted_probs) >= n_days:\n",
    "                metric_value = earn_metric(predicted_probs, progressions, n_days)\n",
    "                metrics.append(metric_value)\n",
    "\n",
    "        avg_metric = np.mean(metrics)\n",
    "        \n",
    "        if avg_metric > best_metric:\n",
    "            best_metric = avg_metric\n",
    "            best_params = {'learning_rate': lr, 'lstm_units': units}\n",
    "\n",
    "print(f\"Best metric: {best_metric:.4f}\")\n",
    "print(f\"Best hyperparameters: {best_params}\")\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"1000 minutes d'exec\n",
    "Best metric: 1.0393\n",
    "Best hyperparameters: {'learning_rate': 0.001, 'lstm_units': 32}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
