{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns=200\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/btc_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crop = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les caractéristiques et exclure la dernière ligne\n",
    "features = data_crop.drop(columns=['progression tomorrow', 'target', 'close', 'high', 'low', 'volumefrom']).iloc[:-1, :]\n",
    "target = data['target'].iloc[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3825"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# 1. Préparation des données\\n\\nX = features.values\\ny = target.values\\n\\nwindow_size = 1000 # Vous pouvez ajuster cette valeur selon vos besoins\\n\\n# 2. Construction du modèle\\n\\nmodel = keras.Sequential([\\n    layers.InputLayer(input_shape=(X.shape[1],)),\\n    layers.Dense(64, activation=\\'relu\\'),\\n    layers.Dense(32, activation=\\'relu\\'),\\n    layers.Dense(1, activation=\\'sigmoid\\')\\n])\\n\\nmodel.compile(optimizer=\\'adam\\', loss=\\'binary_crossentropy\\', metrics=[\\'accuracy\\'])\\n\\n# 3. Entraînement du modèle avec validation walk-forward\\n\\nfor i in range(window_size, len(X) - 1):\\n    # Séparation des données en ensembles d\\'entraînement et de test\\n    X_train = X[i-window_size:i]\\n    y_train = y[i-window_size:i]\\n    X_test = X[i:i+1]\\n    y_test = y[i:i+1]\\n\\n    # Normalisation des caractéristiques\\n    scaler = StandardScaler()\\n    X_train = scaler.fit_transform(X_train)\\n    X_test = scaler.transform(X_test)\\n\\n    # Entraînement du modèle\\n    model.fit(X_train, y_train, epochs=10, verbose=0)  # Vous pouvez ajuster le nombre d\\'epochs selon vos besoins\\n\\n    # Évaluation du modèle (optionnel)\\n    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\\n    print(f\"Step {i}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# 1. Préparation des données\n",
    "\n",
    "X = features.values\n",
    "y = target.values\n",
    "\n",
    "window_size = 1000 # Vous pouvez ajuster cette valeur selon vos besoins\n",
    "\n",
    "# 2. Construction du modèle\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(X.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 3. Entraînement du modèle avec validation walk-forward\n",
    "\n",
    "for i in range(window_size, len(X) - 1):\n",
    "    # Séparation des données en ensembles d'entraînement et de test\n",
    "    X_train = X[i-window_size:i]\n",
    "    y_train = y[i-window_size:i]\n",
    "    X_test = X[i:i+1]\n",
    "    y_test = y[i:i+1]\n",
    "\n",
    "    # Normalisation des caractéristiques\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    model.fit(X_train, y_train, epochs=10, verbose=0)  # Vous pouvez ajuster le nombre d'epochs selon vos besoins\n",
    "\n",
    "    # Évaluation du modèle (optionnel)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Step {i}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Définir la métrique personnalisée\n",
    "\n",
    "def earn_metric(predicted_probs, progressions, n_days):\n",
    "    base = c = 1\n",
    "    for j in range(n_days):\n",
    "        index = len(predicted_probs) - n_days + j\n",
    "        c *= predicted_probs[index] * progressions[index] + (1 - predicted_probs[index])\n",
    "        base *= progressions[index]\n",
    "    return c / base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Préparation des données\\n\\nX = features.values\\ny = target.values\\n\\nwindow_size = 2000  \\nn_days = 30 \\n\\n# Construction du modèle\\n\\nmodel = keras.Sequential([\\n    layers.InputLayer(input_shape=(X.shape[1],)),\\n    layers.Dense(64, activation=\\'relu\\'),\\n    layers.Dense(32, activation=\\'relu\\'),\\n    layers.Dense(1, activation=\\'sigmoid\\')\\n])\\n\\nmodel.compile(optimizer=\\'adam\\', loss=\\'binary_crossentropy\\')\\n\\n# Entraînement du modèle avec validation walk-forward\\n\\npredicted_probs = []\\nprogressions = []\\nmetrics = []\\n\\nfor i in range(window_size, len(X) - 1 - n_days):\\n    # Séparation des données\\n    X_train = X[i-window_size:i]\\n    y_train = y[i-window_size:i]\\n    X_test = X[i:i+n_days]\\n    y_test = y[i:i+n_days]\\n\\n    # Normalisation\\n    scaler = StandardScaler()\\n    X_train = scaler.fit_transform(X_train)\\n    X_test = scaler.transform(X_test)\\n\\n    with HiddenPrints():\\n        # Entraînement\\n        model.fit(X_train, y_train, epochs=20, verbose=0)\\n\\n        # Prédiction\\n        predicted_prob = model.predict(X_test).flatten()\\n        predicted_probs.extend(predicted_prob)\\n\\n    # Récupérer la progression réelle\\n    progression = data_crop.iloc[i:i+n_days][\\'progression tomorrow\\'].values + 1\\n    progressions.extend(progression)\\n\\n    # Calcul de la métrique personnalisée\\n    if len(predicted_probs) >= n_days:\\n        metric_value = earn_metric(predicted_probs, progressions, n_days)\\n        metrics.append(metric_value)\\n        #print(f\"Step {i}, Earn Metric: {metric_value:.4f}\")\\n\\nprint(f\"Average Earn Metric: {np.mean(metrics):.4f}\")'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Préparation des données\n",
    "\n",
    "X = features.values\n",
    "y = target.values\n",
    "\n",
    "window_size = 2000  \n",
    "n_days = 30 \n",
    "\n",
    "# Construction du modèle\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(X.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Entraînement du modèle avec validation walk-forward\n",
    "\n",
    "predicted_probs = []\n",
    "progressions = []\n",
    "metrics = []\n",
    "\n",
    "for i in range(window_size, len(X) - 1 - n_days):\n",
    "    # Séparation des données\n",
    "    X_train = X[i-window_size:i]\n",
    "    y_train = y[i-window_size:i]\n",
    "    X_test = X[i:i+n_days]\n",
    "    y_test = y[i:i+n_days]\n",
    "\n",
    "    # Normalisation\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    with HiddenPrints():\n",
    "        # Entraînement\n",
    "        model.fit(X_train, y_train, epochs=20, verbose=0)\n",
    "\n",
    "        # Prédiction\n",
    "        predicted_prob = model.predict(X_test).flatten()\n",
    "        predicted_probs.extend(predicted_prob)\n",
    "\n",
    "    # Récupérer la progression réelle\n",
    "    progression = data_crop.iloc[i:i+n_days]['progression tomorrow'].values + 1\n",
    "    progressions.extend(progression)\n",
    "\n",
    "    # Calcul de la métrique personnalisée\n",
    "    if len(predicted_probs) >= n_days:\n",
    "        metric_value = earn_metric(predicted_probs, progressions, n_days)\n",
    "        metrics.append(metric_value)\n",
    "        #print(f\"Step {i}, Earn Metric: {metric_value:.4f}\")\n",
    "\n",
    "print(f\"Average Earn Metric: {np.mean(metrics):.4f}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Earn Metric: 0.9799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import numpy as np\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# Préparation des données\\n\\nX = features.values\\ny = target.values\\n\\nwindow_size = 3000  \\nn_days = 30 \\n\\n# Construction du modèle LSTM\\n\\nmodel = keras.Sequential([\\n    layers.InputLayer(input_shape=(X.shape[1], 1)),  # Les LSTM attendent une entrée tridimensionnelle (batch_size, timesteps, features)\\n    layers.LSTM(64, return_sequences=True),  # Retourne une séquence à la couche suivante\\n    layers.LSTM(32),  # Retourne la dernière sortie de la séquence\\n    layers.Dense(1, activation=\\'sigmoid\\')\\n])\\n\\nmodel.compile(optimizer=\\'adam\\', loss=\\'binary_crossentropy\\')\\n\\n# Entraînement du modèle avec validation walk-forward\\n\\npredicted_probs = []\\nprogressions = []\\nmetrics = []\\n\\nfor i in range(window_size, len(X) - 1 - n_days):\\n    # Séparation des données\\n    X_train = X[i-window_size:i]\\n    y_train = y[i-window_size:i]\\n    X_test = X[i:i+n_days]\\n    y_test = y[i:i+n_days]\\n\\n    # Normalisation\\n    scaler = StandardScaler()\\n    X_train = scaler.fit_transform(X_train)\\n    X_test = scaler.transform(X_test)\\n\\n    # Les LSTM attendent une entrée tridimensionnelle, donc nous devons remodeler les données\\n    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\\n    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\\n\\n    with HiddenPrints():\\n        # Entraînement\\n        model.fit(X_train, y_train, epochs=10, verbose=0)\\n\\n        # Prédiction\\n        predicted_prob = model.predict(X_test).flatten()\\n        predicted_probs.extend(predicted_prob)\\n\\n    # Récupérer la progression réelle\\n    progression = data_crop.iloc[i:i+n_days][\\'progression tomorrow\\'].values + 1\\n    progressions.extend(progression)\\n\\n    # Calcul de la métrique personnalisée\\n    if len(predicted_probs) >= n_days:\\n        metric_value = earn_metric(predicted_probs, progressions, n_days)\\n        metrics.append(metric_value)\\n\\nprint(f\"Average Earn Metric: {np.mean(metrics):.4f}\")'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Préparation des données\n",
    "\n",
    "X = features.values\n",
    "y = target.values\n",
    "\n",
    "window_size = 3000  \n",
    "n_days = 30 \n",
    "\n",
    "# Construction du modèle LSTM\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(X.shape[1], 1)),  # Les LSTM attendent une entrée tridimensionnelle (batch_size, timesteps, features)\n",
    "    layers.LSTM(64, return_sequences=True),  # Retourne une séquence à la couche suivante\n",
    "    layers.LSTM(32),  # Retourne la dernière sortie de la séquence\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# Entraînement du modèle avec validation walk-forward\n",
    "\n",
    "predicted_probs = []\n",
    "progressions = []\n",
    "metrics = []\n",
    "\n",
    "for i in range(window_size, len(X) - 1 - n_days):\n",
    "    # Séparation des données\n",
    "    X_train = X[i-window_size:i]\n",
    "    y_train = y[i-window_size:i]\n",
    "    X_test = X[i:i+n_days]\n",
    "    y_test = y[i:i+n_days]\n",
    "\n",
    "    # Normalisation\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Les LSTM attendent une entrée tridimensionnelle, donc nous devons remodeler les données\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    with HiddenPrints():\n",
    "        # Entraînement\n",
    "        model.fit(X_train, y_train, epochs=10, verbose=0)\n",
    "\n",
    "        # Prédiction\n",
    "        predicted_prob = model.predict(X_test).flatten()\n",
    "        predicted_probs.extend(predicted_prob)\n",
    "\n",
    "    # Récupérer la progression réelle\n",
    "    progression = data_crop.iloc[i:i+n_days]['progression tomorrow'].values + 1\n",
    "    progressions.extend(progression)\n",
    "\n",
    "    # Calcul de la métrique personnalisée\n",
    "    if len(predicted_probs) >= n_days:\n",
    "        metric_value = earn_metric(predicted_probs, progressions, n_days)\n",
    "        metrics.append(metric_value)\n",
    "\n",
    "print(f\"Average Earn Metric: {np.mean(metrics):.4f}\")\"\"\"\n",
    "\n",
    "#2h15 pour un average metric à 1.0055... (epoch 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1000 minutes d'exec\\nBest metric: 1.0393\\nBest hyperparameters: {'learning_rate': 0.001, 'lstm_units': 32}\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Préparation des données\n",
    "X = features.values\n",
    "y = target.values\n",
    "\n",
    "window_size = 3000  \n",
    "n_days = 30 \n",
    "\n",
    "best_metric = 0\n",
    "best_params = None\n",
    "\n",
    "# Espace de recherche\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "lstm_units = [32, 64, 128]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for units in lstm_units:\n",
    "        # Construction du modèle\n",
    "        model = keras.Sequential([\n",
    "            layers.InputLayer(input_shape=(X.shape[1], 1)),\n",
    "            layers.LSTM(units, return_sequences=True),\n",
    "            layers.LSTM(units),\n",
    "            layers.Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "        \n",
    "        # Entraînement du modèle avec validation walk-forward\n",
    "        predicted_probs = []\n",
    "        progressions = []\n",
    "        metrics = []\n",
    "\n",
    "        for i in range(window_size, len(X) - 1 - n_days):\n",
    "            # Séparation des données\n",
    "            X_train = X[i-window_size:i]\n",
    "            y_train = y[i-window_size:i]\n",
    "            X_test = X[i:i+n_days]\n",
    "            y_test = y[i:i+n_days]\n",
    "\n",
    "            # Normalisation\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            # Les LSTM attendent une entrée tridimensionnelle\n",
    "            X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "            X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "\n",
    "            with HiddenPrints():\n",
    "                # Entraînement\n",
    "                model.fit(X_train, y_train, epochs=10, verbose=0)\n",
    "\n",
    "                # Prédiction\n",
    "                predicted_prob = model.predict(X_test).flatten()\n",
    "                predicted_probs.extend(predicted_prob)\n",
    "\n",
    "            # Récupérer la progression réelle\n",
    "            progression = data_crop.iloc[i:i+n_days]['progression tomorrow'].values + 1\n",
    "            progressions.extend(progression)\n",
    "\n",
    "            # Calcul de la métrique personnalisée\n",
    "            if len(predicted_probs) >= n_days:\n",
    "                metric_value = earn_metric(predicted_probs, progressions, n_days)\n",
    "                metrics.append(metric_value)\n",
    "\n",
    "        avg_metric = np.mean(metrics)\n",
    "        \n",
    "        if avg_metric > best_metric:\n",
    "            best_metric = avg_metric\n",
    "            best_params = {'learning_rate': lr, 'lstm_units': units}\n",
    "\n",
    "print(f\"Best metric: {best_metric:.4f}\")\n",
    "print(f\"Best hyperparameters: {best_params}\")\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"1000 minutes d'exec\n",
    "Best metric: 1.0393\n",
    "Best hyperparameters: {'learning_rate': 0.001, 'lstm_units': 32}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy 0.5369'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Préparation des données\n",
    "\n",
    "X = features.values\n",
    "y = target.values\n",
    "\n",
    "window_size = 3000  \n",
    "n_days = 30 \n",
    "\n",
    "# Construction du modèle LSTM avec les hyperparamètres sélectionnés\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(X.shape[1], 1)),  # Les LSTM attendent une entrée tridimensionnelle\n",
    "    layers.LSTM(32, return_sequences=True),  # Utilisation de 32 unités comme déterminé par la recherche d'hyperparamètres\n",
    "    layers.LSTM(32),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Définir l'early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Entraînement du modèle avec validation walk-forward et early stopping\n",
    "predicted_probs = []\n",
    "progressions = []\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "\n",
    "for i in range(window_size, len(X) - 1 - n_days):\n",
    "    # Séparation des données\n",
    "    X_train = X[i-window_size:i]\n",
    "    y_train = y[i-window_size:i]\n",
    "    X_test = X[i:i+n_days]\n",
    "    y_test = y[i:i+n_days]\n",
    "\n",
    "    # Normalisation\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Les LSTM attendent une entrée tridimensionnelle, donc nous devons remodeler les données\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "    with HiddenPrints():\n",
    "        # Entraînement\n",
    "        model.fit(X_train, y_train, epochs=30, verbose=0, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "        # Prédiction\n",
    "        predicted_prob = model.predict(X_test).flatten()\n",
    "        predicted_label = [1 if prob > 0.5 else 0 for prob in predicted_prob]\n",
    "        predicted_labels.extend(predicted_label)\n",
    "        true_labels.extend(y_test)\n",
    "\n",
    "# Calcul de l'accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\"\"\"\n",
    "\n",
    "\"Accuracy 0.5369\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Effectuer une PCA\n",
    "pca = PCA().fit(features)\n",
    "\n",
    "# Tracer la variance expliquée cumulée\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)')\n",
    "plt.title('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>volumeto</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>progression daily</th>\n",
       "      <th>ema_26</th>\n",
       "      <th>ema_12</th>\n",
       "      <th>macd</th>\n",
       "      <th>rsi</th>\n",
       "      <th>relative_volume</th>\n",
       "      <th>obv</th>\n",
       "      <th>atr</th>\n",
       "      <th>bollinger_upper</th>\n",
       "      <th>bollinger_lower</th>\n",
       "      <th>k</th>\n",
       "      <th>momentum</th>\n",
       "      <th>difficulty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.724425</td>\n",
       "      <td>0.999673</td>\n",
       "      <td>-0.010186</td>\n",
       "      <td>0.992889</td>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.157425</td>\n",
       "      <td>-0.024460</td>\n",
       "      <td>-0.012519</td>\n",
       "      <td>0.859309</td>\n",
       "      <td>0.893863</td>\n",
       "      <td>0.991260</td>\n",
       "      <td>0.986485</td>\n",
       "      <td>-0.010248</td>\n",
       "      <td>0.077930</td>\n",
       "      <td>0.692039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volumeto</th>\n",
       "      <td>0.724425</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.717525</td>\n",
       "      <td>-0.017553</td>\n",
       "      <td>0.717025</td>\n",
       "      <td>0.723973</td>\n",
       "      <td>0.181848</td>\n",
       "      <td>0.015977</td>\n",
       "      <td>0.271129</td>\n",
       "      <td>0.728472</td>\n",
       "      <td>0.804493</td>\n",
       "      <td>0.740401</td>\n",
       "      <td>0.685379</td>\n",
       "      <td>0.015622</td>\n",
       "      <td>0.046104</td>\n",
       "      <td>0.419655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>market_cap</th>\n",
       "      <td>0.999673</td>\n",
       "      <td>0.717525</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.010996</td>\n",
       "      <td>0.993086</td>\n",
       "      <td>0.997101</td>\n",
       "      <td>0.152752</td>\n",
       "      <td>-0.026446</td>\n",
       "      <td>-0.012770</td>\n",
       "      <td>0.851525</td>\n",
       "      <td>0.888209</td>\n",
       "      <td>0.990640</td>\n",
       "      <td>0.987468</td>\n",
       "      <td>-0.013541</td>\n",
       "      <td>0.075038</td>\n",
       "      <td>0.703429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>progression daily</th>\n",
       "      <td>-0.010186</td>\n",
       "      <td>-0.017553</td>\n",
       "      <td>-0.010996</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.025330</td>\n",
       "      <td>-0.023701</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>0.159195</td>\n",
       "      <td>0.018530</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>-0.016846</td>\n",
       "      <td>-0.023761</td>\n",
       "      <td>-0.027572</td>\n",
       "      <td>0.226594</td>\n",
       "      <td>0.108989</td>\n",
       "      <td>-0.022322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ema_26</th>\n",
       "      <td>0.992889</td>\n",
       "      <td>0.717025</td>\n",
       "      <td>0.993086</td>\n",
       "      <td>-0.025330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998403</td>\n",
       "      <td>0.054120</td>\n",
       "      <td>-0.085833</td>\n",
       "      <td>-0.020268</td>\n",
       "      <td>0.838178</td>\n",
       "      <td>0.900118</td>\n",
       "      <td>0.996532</td>\n",
       "      <td>0.994642</td>\n",
       "      <td>-0.067880</td>\n",
       "      <td>-0.025736</td>\n",
       "      <td>0.700732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ema_12</th>\n",
       "      <td>0.997169</td>\n",
       "      <td>0.723973</td>\n",
       "      <td>0.997101</td>\n",
       "      <td>-0.023701</td>\n",
       "      <td>0.998403</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.110442</td>\n",
       "      <td>-0.061301</td>\n",
       "      <td>-0.016846</td>\n",
       "      <td>0.849906</td>\n",
       "      <td>0.899730</td>\n",
       "      <td>0.996170</td>\n",
       "      <td>0.993245</td>\n",
       "      <td>-0.049990</td>\n",
       "      <td>0.010596</td>\n",
       "      <td>0.696223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macd</th>\n",
       "      <td>0.157425</td>\n",
       "      <td>0.181848</td>\n",
       "      <td>0.152752</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>0.054120</td>\n",
       "      <td>0.110442</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.426549</td>\n",
       "      <td>0.058818</td>\n",
       "      <td>0.276327</td>\n",
       "      <td>0.067248</td>\n",
       "      <td>0.075663</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.310638</td>\n",
       "      <td>0.640094</td>\n",
       "      <td>-0.021995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rsi</th>\n",
       "      <td>-0.024460</td>\n",
       "      <td>0.015977</td>\n",
       "      <td>-0.026446</td>\n",
       "      <td>0.159195</td>\n",
       "      <td>-0.085833</td>\n",
       "      <td>-0.061301</td>\n",
       "      <td>0.426549</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.161767</td>\n",
       "      <td>0.053081</td>\n",
       "      <td>-0.076028</td>\n",
       "      <td>-0.081514</td>\n",
       "      <td>-0.096534</td>\n",
       "      <td>0.741827</td>\n",
       "      <td>0.487110</td>\n",
       "      <td>-0.080912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relative_volume</th>\n",
       "      <td>-0.012519</td>\n",
       "      <td>0.271129</td>\n",
       "      <td>-0.012770</td>\n",
       "      <td>0.018530</td>\n",
       "      <td>-0.020268</td>\n",
       "      <td>-0.016846</td>\n",
       "      <td>0.058818</td>\n",
       "      <td>0.161767</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>-0.032592</td>\n",
       "      <td>-0.023419</td>\n",
       "      <td>-0.017760</td>\n",
       "      <td>0.086563</td>\n",
       "      <td>0.067285</td>\n",
       "      <td>-0.019540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obv</th>\n",
       "      <td>0.859309</td>\n",
       "      <td>0.728472</td>\n",
       "      <td>0.851525</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.838178</td>\n",
       "      <td>0.849906</td>\n",
       "      <td>0.276327</td>\n",
       "      <td>0.053081</td>\n",
       "      <td>0.005173</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.844573</td>\n",
       "      <td>0.852100</td>\n",
       "      <td>0.819689</td>\n",
       "      <td>0.055826</td>\n",
       "      <td>0.154347</td>\n",
       "      <td>0.502792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atr</th>\n",
       "      <td>0.893863</td>\n",
       "      <td>0.804493</td>\n",
       "      <td>0.888209</td>\n",
       "      <td>-0.016846</td>\n",
       "      <td>0.900118</td>\n",
       "      <td>0.899730</td>\n",
       "      <td>0.067248</td>\n",
       "      <td>-0.076028</td>\n",
       "      <td>-0.032592</td>\n",
       "      <td>0.844573</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925076</td>\n",
       "      <td>0.864667</td>\n",
       "      <td>-0.043449</td>\n",
       "      <td>-0.047399</td>\n",
       "      <td>0.477496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bollinger_upper</th>\n",
       "      <td>0.991260</td>\n",
       "      <td>0.740401</td>\n",
       "      <td>0.990640</td>\n",
       "      <td>-0.023761</td>\n",
       "      <td>0.996532</td>\n",
       "      <td>0.996170</td>\n",
       "      <td>0.075663</td>\n",
       "      <td>-0.081514</td>\n",
       "      <td>-0.023419</td>\n",
       "      <td>0.852100</td>\n",
       "      <td>0.925076</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984654</td>\n",
       "      <td>-0.062569</td>\n",
       "      <td>-0.022387</td>\n",
       "      <td>0.679401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bollinger_lower</th>\n",
       "      <td>0.986485</td>\n",
       "      <td>0.685379</td>\n",
       "      <td>0.987468</td>\n",
       "      <td>-0.027572</td>\n",
       "      <td>0.994642</td>\n",
       "      <td>0.993245</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>-0.096534</td>\n",
       "      <td>-0.017760</td>\n",
       "      <td>0.819689</td>\n",
       "      <td>0.864667</td>\n",
       "      <td>0.984654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.081459</td>\n",
       "      <td>-0.044864</td>\n",
       "      <td>0.715190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <td>-0.010248</td>\n",
       "      <td>0.015622</td>\n",
       "      <td>-0.013541</td>\n",
       "      <td>0.226594</td>\n",
       "      <td>-0.067880</td>\n",
       "      <td>-0.049990</td>\n",
       "      <td>0.310638</td>\n",
       "      <td>0.741827</td>\n",
       "      <td>0.086563</td>\n",
       "      <td>0.055826</td>\n",
       "      <td>-0.043449</td>\n",
       "      <td>-0.062569</td>\n",
       "      <td>-0.081459</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.499241</td>\n",
       "      <td>-0.090765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>momentum</th>\n",
       "      <td>0.077930</td>\n",
       "      <td>0.046104</td>\n",
       "      <td>0.075038</td>\n",
       "      <td>0.108989</td>\n",
       "      <td>-0.025736</td>\n",
       "      <td>0.010596</td>\n",
       "      <td>0.640094</td>\n",
       "      <td>0.487110</td>\n",
       "      <td>0.067285</td>\n",
       "      <td>0.154347</td>\n",
       "      <td>-0.047399</td>\n",
       "      <td>-0.022387</td>\n",
       "      <td>-0.044864</td>\n",
       "      <td>0.499241</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>difficulty</th>\n",
       "      <td>0.692039</td>\n",
       "      <td>0.419655</td>\n",
       "      <td>0.703429</td>\n",
       "      <td>-0.022322</td>\n",
       "      <td>0.700732</td>\n",
       "      <td>0.696223</td>\n",
       "      <td>-0.021995</td>\n",
       "      <td>-0.080912</td>\n",
       "      <td>-0.019540</td>\n",
       "      <td>0.502792</td>\n",
       "      <td>0.477496</td>\n",
       "      <td>0.679401</td>\n",
       "      <td>0.715190</td>\n",
       "      <td>-0.090765</td>\n",
       "      <td>-0.021757</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       open  volumeto  market_cap  progression daily  \\\n",
       "open               1.000000  0.724425    0.999673          -0.010186   \n",
       "volumeto           0.724425  1.000000    0.717525          -0.017553   \n",
       "market_cap         0.999673  0.717525    1.000000          -0.010996   \n",
       "progression daily -0.010186 -0.017553   -0.010996           1.000000   \n",
       "ema_26             0.992889  0.717025    0.993086          -0.025330   \n",
       "ema_12             0.997169  0.723973    0.997101          -0.023701   \n",
       "macd               0.157425  0.181848    0.152752           0.026697   \n",
       "rsi               -0.024460  0.015977   -0.026446           0.159195   \n",
       "relative_volume   -0.012519  0.271129   -0.012770           0.018530   \n",
       "obv                0.859309  0.728472    0.851525           0.000204   \n",
       "atr                0.893863  0.804493    0.888209          -0.016846   \n",
       "bollinger_upper    0.991260  0.740401    0.990640          -0.023761   \n",
       "bollinger_lower    0.986485  0.685379    0.987468          -0.027572   \n",
       "k                 -0.010248  0.015622   -0.013541           0.226594   \n",
       "momentum           0.077930  0.046104    0.075038           0.108989   \n",
       "difficulty         0.692039  0.419655    0.703429          -0.022322   \n",
       "\n",
       "                     ema_26    ema_12      macd       rsi  relative_volume  \\\n",
       "open               0.992889  0.997169  0.157425 -0.024460        -0.012519   \n",
       "volumeto           0.717025  0.723973  0.181848  0.015977         0.271129   \n",
       "market_cap         0.993086  0.997101  0.152752 -0.026446        -0.012770   \n",
       "progression daily -0.025330 -0.023701  0.026697  0.159195         0.018530   \n",
       "ema_26             1.000000  0.998403  0.054120 -0.085833        -0.020268   \n",
       "ema_12             0.998403  1.000000  0.110442 -0.061301        -0.016846   \n",
       "macd               0.054120  0.110442  1.000000  0.426549         0.058818   \n",
       "rsi               -0.085833 -0.061301  0.426549  1.000000         0.161767   \n",
       "relative_volume   -0.020268 -0.016846  0.058818  0.161767         1.000000   \n",
       "obv                0.838178  0.849906  0.276327  0.053081         0.005173   \n",
       "atr                0.900118  0.899730  0.067248 -0.076028        -0.032592   \n",
       "bollinger_upper    0.996532  0.996170  0.075663 -0.081514        -0.023419   \n",
       "bollinger_lower    0.994642  0.993245  0.057203 -0.096534        -0.017760   \n",
       "k                 -0.067880 -0.049990  0.310638  0.741827         0.086563   \n",
       "momentum          -0.025736  0.010596  0.640094  0.487110         0.067285   \n",
       "difficulty         0.700732  0.696223 -0.021995 -0.080912        -0.019540   \n",
       "\n",
       "                        obv       atr  bollinger_upper  bollinger_lower  \\\n",
       "open               0.859309  0.893863         0.991260         0.986485   \n",
       "volumeto           0.728472  0.804493         0.740401         0.685379   \n",
       "market_cap         0.851525  0.888209         0.990640         0.987468   \n",
       "progression daily  0.000204 -0.016846        -0.023761        -0.027572   \n",
       "ema_26             0.838178  0.900118         0.996532         0.994642   \n",
       "ema_12             0.849906  0.899730         0.996170         0.993245   \n",
       "macd               0.276327  0.067248         0.075663         0.057203   \n",
       "rsi                0.053081 -0.076028        -0.081514        -0.096534   \n",
       "relative_volume    0.005173 -0.032592        -0.023419        -0.017760   \n",
       "obv                1.000000  0.844573         0.852100         0.819689   \n",
       "atr                0.844573  1.000000         0.925076         0.864667   \n",
       "bollinger_upper    0.852100  0.925076         1.000000         0.984654   \n",
       "bollinger_lower    0.819689  0.864667         0.984654         1.000000   \n",
       "k                  0.055826 -0.043449        -0.062569        -0.081459   \n",
       "momentum           0.154347 -0.047399        -0.022387        -0.044864   \n",
       "difficulty         0.502792  0.477496         0.679401         0.715190   \n",
       "\n",
       "                          k  momentum  difficulty  \n",
       "open              -0.010248  0.077930    0.692039  \n",
       "volumeto           0.015622  0.046104    0.419655  \n",
       "market_cap        -0.013541  0.075038    0.703429  \n",
       "progression daily  0.226594  0.108989   -0.022322  \n",
       "ema_26            -0.067880 -0.025736    0.700732  \n",
       "ema_12            -0.049990  0.010596    0.696223  \n",
       "macd               0.310638  0.640094   -0.021995  \n",
       "rsi                0.741827  0.487110   -0.080912  \n",
       "relative_volume    0.086563  0.067285   -0.019540  \n",
       "obv                0.055826  0.154347    0.502792  \n",
       "atr               -0.043449 -0.047399    0.477496  \n",
       "bollinger_upper   -0.062569 -0.022387    0.679401  \n",
       "bollinger_lower   -0.081459 -0.044864    0.715190  \n",
       "k                  1.000000  0.499241   -0.090765  \n",
       "momentum           0.499241  1.000000   -0.021757  \n",
       "difficulty        -0.090765 -0.021757    1.000000  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculer la matrice de corrélation\n",
    "corr_matrix = features.corr()\n",
    "\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision_value = precision(y_true, y_pred)\n",
    "    recall_value = recall(y_true, y_pred)\n",
    "    return 2 * ((precision_value * recall_value) / (precision_value + recall_value + K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 3s 11ms/step - loss: 0.6969 - accuracy: 0.4552 - f1_score: 0.4427\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6906 - accuracy: 0.4925 - f1_score: 0.4769\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6871 - accuracy: 0.5448 - f1_score: 0.5481\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6819 - accuracy: 0.5597 - f1_score: 0.5693\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6782 - accuracy: 0.5597 - f1_score: 0.5693\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6754 - accuracy: 0.5597 - f1_score: 0.5693\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6728 - accuracy: 0.5597 - f1_score: 0.5693\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6717 - accuracy: 0.5672 - f1_score: 0.5797\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6696 - accuracy: 0.5746 - f1_score: 0.5899\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6680 - accuracy: 0.5746 - f1_score: 0.5899\n",
      "4/4 [==============================] - 1s 5ms/step - loss: 0.7492 - accuracy: 0.4228 - f1_score: 0.5943\n",
      "Loss for split: [0.7492153644561768, 0.42276424169540405, array([0.5942857], dtype=float32)]\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.7188 - accuracy: 0.4630 - f1_score: 0.3100\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.7081 - accuracy: 0.4591 - f1_score: 0.3220\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 0s 14ms/step - loss: 0.6981 - accuracy: 0.4864 - f1_score: 0.3529\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6934 - accuracy: 0.5214 - f1_score: 0.3560\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 0s 8ms/step - loss: 0.6890 - accuracy: 0.5486 - f1_score: 0.4257\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6882 - accuracy: 0.5447 - f1_score: 0.4658\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6871 - accuracy: 0.5486 - f1_score: 0.4476\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6871 - accuracy: 0.5564 - f1_score: 0.4356\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 0s 10ms/step - loss: 0.6871 - accuracy: 0.5564 - f1_score: 0.5043\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 0s 9ms/step - loss: 0.6864 - accuracy: 0.5525 - f1_score: 0.5525\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6791 - accuracy: 0.5285 - f1_score: 0.1714\n",
      "Loss for split: [0.6790701746940613, 0.5284552574157715, array([0.17142858], dtype=float32)]\n",
      "Epoch 1/10\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6846 - accuracy: 0.5632 - f1_score: 0.4276\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6835 - accuracy: 0.5763 - f1_score: 0.4790\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 0.6825 - accuracy: 0.5737 - f1_score: 0.5714\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6822 - accuracy: 0.5684 - f1_score: 0.5879\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6819 - accuracy: 0.5684 - f1_score: 0.5859\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6810 - accuracy: 0.5658 - f1_score: 0.5528\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6813 - accuracy: 0.5711 - f1_score: 0.5303\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 0.6808 - accuracy: 0.5842 - f1_score: 0.5990\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6800 - accuracy: 0.5684 - f1_score: 0.5900\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 0.6802 - accuracy: 0.5658 - f1_score: 0.5299\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6884 - accuracy: 0.4878 - f1_score: 0.4324\n",
      "Loss for split: [0.6884311437606812, 0.4878048896789551, array([0.4324324], dtype=float32)]\n",
      "Epoch 1/10\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6811 - accuracy: 0.5805 - f1_score: 0.5383\n",
      "Epoch 2/10\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6830 - accuracy: 0.5527 - f1_score: 0.5731\n",
      "Epoch 3/10\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6821 - accuracy: 0.5706 - f1_score: 0.5814\n",
      "Epoch 4/10\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6808 - accuracy: 0.5746 - f1_score: 0.5771\n",
      "Epoch 5/10\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6800 - accuracy: 0.5746 - f1_score: 0.5388\n",
      "Epoch 6/10\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.6807 - accuracy: 0.5646 - f1_score: 0.5144\n",
      "Epoch 7/10\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.6801 - accuracy: 0.5706 - f1_score: 0.5610\n",
      "Epoch 8/10\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6819 - accuracy: 0.5805 - f1_score: 0.5539\n",
      "Epoch 9/10\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6812 - accuracy: 0.5507 - f1_score: 0.5462\n",
      "Epoch 10/10\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.6801 - accuracy: 0.5666 - f1_score: 0.5742\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6810 - accuracy: 0.6098 - f1_score: 0.7551\n",
      "Loss for split: [0.6809640526771545, 0.6097561120986938, array([0.75510204], dtype=float32)]\n",
      "Epoch 1/10\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.6799 - accuracy: 0.5639 - f1_score: 0.6094\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.6791 - accuracy: 0.5927 - f1_score: 0.6200\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.6794 - accuracy: 0.5863 - f1_score: 0.6337\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.6795 - accuracy: 0.5735 - f1_score: 0.6473\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.6800 - accuracy: 0.5847 - f1_score: 0.6275\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.6793 - accuracy: 0.5783 - f1_score: 0.6452\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 0s 10ms/step - loss: 0.6794 - accuracy: 0.5719 - f1_score: 0.6309\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.6787 - accuracy: 0.5863 - f1_score: 0.6428\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.6788 - accuracy: 0.5719 - f1_score: 0.6349\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 0s 9ms/step - loss: 0.6792 - accuracy: 0.5751 - f1_score: 0.6376\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6993 - accuracy: 0.5041 - f1_score: 0.6703\n",
      "Loss for split: [0.6993038654327393, 0.5040650367736816, array([0.6702703], dtype=float32)]\n",
      "Epoch 1/10\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6827 - accuracy: 0.5754 - f1_score: 0.6311\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6821 - accuracy: 0.5621 - f1_score: 0.6281\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6819 - accuracy: 0.5781 - f1_score: 0.6376\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6816 - accuracy: 0.5754 - f1_score: 0.6205\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6822 - accuracy: 0.5634 - f1_score: 0.6403\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 10ms/step - loss: 0.6821 - accuracy: 0.5808 - f1_score: 0.6456\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6820 - accuracy: 0.5728 - f1_score: 0.6437\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6825 - accuracy: 0.5781 - f1_score: 0.6300\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.6828 - accuracy: 0.5768 - f1_score: 0.6513\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6818 - accuracy: 0.5648 - f1_score: 0.6472\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6654 - accuracy: 0.6341 - f1_score: 0.7739\n",
      "Loss for split: [0.6654093861579895, 0.6341463327407837, array([0.77386934], dtype=float32)]\n",
      "Epoch 1/10\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6795 - accuracy: 0.5757 - f1_score: 0.6673\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6780 - accuracy: 0.5791 - f1_score: 0.6800\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6783 - accuracy: 0.5814 - f1_score: 0.6721\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6785 - accuracy: 0.5849 - f1_score: 0.6623\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6780 - accuracy: 0.5940 - f1_score: 0.6722\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 0.6786 - accuracy: 0.5826 - f1_score: 0.6824\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6788 - accuracy: 0.5768 - f1_score: 0.6860\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6785 - accuracy: 0.5837 - f1_score: 0.6762\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6782 - accuracy: 0.5791 - f1_score: 0.6605\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 0.6780 - accuracy: 0.5837 - f1_score: 0.6721\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.7210 - accuracy: 0.4715 - f1_score: 0.4961\n",
      "Loss for split: [0.721023440361023, 0.47154471278190613, array([0.49612403], dtype=float32)]\n",
      "Epoch 1/10\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.6831 - accuracy: 0.5608 - f1_score: 0.6646\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.6839 - accuracy: 0.5628 - f1_score: 0.6692\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.6834 - accuracy: 0.5688 - f1_score: 0.6560\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.6832 - accuracy: 0.5789 - f1_score: 0.6645\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.6824 - accuracy: 0.5668 - f1_score: 0.6677\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.6822 - accuracy: 0.5648 - f1_score: 0.6692\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.6824 - accuracy: 0.5739 - f1_score: 0.6646\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.6821 - accuracy: 0.5739 - f1_score: 0.6592\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.6821 - accuracy: 0.5618 - f1_score: 0.6692\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 0.6824 - accuracy: 0.5739 - f1_score: 0.6693\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6758 - accuracy: 0.5935 - f1_score: 0.6795\n",
      "Loss for split: [0.6757748126983643, 0.5934959053993225, array([0.67948717], dtype=float32)]\n",
      "Epoch 1/10\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.6805 - accuracy: 0.5796 - f1_score: 0.6871\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.6803 - accuracy: 0.5716 - f1_score: 0.6834\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.6795 - accuracy: 0.5805 - f1_score: 0.6820\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.6799 - accuracy: 0.5769 - f1_score: 0.6836\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.6800 - accuracy: 0.5671 - f1_score: 0.6769\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.6792 - accuracy: 0.5814 - f1_score: 0.6816\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.6791 - accuracy: 0.5805 - f1_score: 0.6837\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.6789 - accuracy: 0.5733 - f1_score: 0.6835\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 0s 8ms/step - loss: 0.6785 - accuracy: 0.5769 - f1_score: 0.6840\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 0.6790 - accuracy: 0.5644 - f1_score: 0.6676\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6875 - accuracy: 0.5772 - f1_score: 0.0714\n",
      "Loss for split: [0.6874621510505676, 0.577235758304596, array([0.07142857], dtype=float32)]\n",
      "Epoch 1/10\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 0.6796 - accuracy: 0.5802 - f1_score: 0.6705\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.6787 - accuracy: 0.5729 - f1_score: 0.6720\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.6782 - accuracy: 0.5826 - f1_score: 0.6667\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.6781 - accuracy: 0.5761 - f1_score: 0.6602\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.6788 - accuracy: 0.5770 - f1_score: 0.6692\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.6786 - accuracy: 0.5778 - f1_score: 0.6769\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.6788 - accuracy: 0.5794 - f1_score: 0.6538\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.6781 - accuracy: 0.5794 - f1_score: 0.6709\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.6779 - accuracy: 0.5842 - f1_score: 0.6636\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.6780 - accuracy: 0.5753 - f1_score: 0.6654\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6888 - accuracy: 0.5610 - f1_score: 0.1000\n",
      "Loss for split: [0.6887739896774292, 0.5609756112098694, array([0.1], dtype=float32)]\n",
      "Epoch 1/10\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.6787 - accuracy: 0.5843 - f1_score: 0.6578\n",
      "Epoch 2/10\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.6789 - accuracy: 0.5784 - f1_score: 0.6571\n",
      "Epoch 3/10\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 0.6783 - accuracy: 0.5792 - f1_score: 0.6542\n",
      "Epoch 4/10\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.6784 - accuracy: 0.5726 - f1_score: 0.6540\n",
      "Epoch 5/10\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.6786 - accuracy: 0.5799 - f1_score: 0.6452\n",
      "Epoch 6/10\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.6786 - accuracy: 0.5806 - f1_score: 0.6469\n",
      "Epoch 7/10\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 0.6774 - accuracy: 0.5850 - f1_score: 0.6549\n",
      "Epoch 8/10\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.6776 - accuracy: 0.5799 - f1_score: 0.6607\n",
      "Epoch 9/10\n",
      "46/46 [==============================] - 0s 8ms/step - loss: 0.6776 - accuracy: 0.5843 - f1_score: 0.6574\n",
      "Epoch 10/10\n",
      "46/46 [==============================] - 0s 9ms/step - loss: 0.6774 - accuracy: 0.5880 - f1_score: 0.6586\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6959 - accuracy: 0.5285 - f1_score: 0.0000e+00\n",
      "Loss for split: [0.6958651542663574, 0.5284552574157715, array([0.], dtype=float32)]\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.6813 - accuracy: 0.5662 - f1_score: 0.6346\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.6802 - accuracy: 0.5750 - f1_score: 0.6405\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.6796 - accuracy: 0.5757 - f1_score: 0.6350\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.6800 - accuracy: 0.5696 - f1_score: 0.6283\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.6799 - accuracy: 0.5723 - f1_score: 0.6370\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.6797 - accuracy: 0.5750 - f1_score: 0.6300\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.6809 - accuracy: 0.5750 - f1_score: 0.6493\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 0.6794 - accuracy: 0.5817 - f1_score: 0.6258\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.6793 - accuracy: 0.5763 - f1_score: 0.6396\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 0s 9ms/step - loss: 0.6790 - accuracy: 0.5689 - f1_score: 0.6297\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4634 - f1_score: 0.5217\n",
      "Loss for split: [0.6932034492492676, 0.46341463923454285, array([0.5217391], dtype=float32)]\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6893 - accuracy: 0.5404 - f1_score: 0.6307\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6877 - accuracy: 0.5466 - f1_score: 0.6117\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.6875 - accuracy: 0.5484 - f1_score: 0.6251\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.6867 - accuracy: 0.5484 - f1_score: 0.6255\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.6860 - accuracy: 0.5540 - f1_score: 0.6337\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6861 - accuracy: 0.5578 - f1_score: 0.6079\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 0s 9ms/step - loss: 0.6855 - accuracy: 0.5540 - f1_score: 0.6127\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 1s 9ms/step - loss: 0.6856 - accuracy: 0.5565 - f1_score: 0.6194\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6855 - accuracy: 0.5596 - f1_score: 0.6107\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 0s 8ms/step - loss: 0.6851 - accuracy: 0.5559 - f1_score: 0.6091\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6970 - accuracy: 0.4146 - f1_score: 0.5443\n",
      "Loss for split: [0.6970213055610657, 0.4146341383457184, array([0.54430383], dtype=float32)]\n",
      "Epoch 1/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 0.6922 - accuracy: 0.5320 - f1_score: 0.5890\n",
      "Epoch 2/10\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6911 - accuracy: 0.5349 - f1_score: 0.6162\n",
      "Epoch 3/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 0.6911 - accuracy: 0.5332 - f1_score: 0.6249\n",
      "Epoch 4/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 0.6910 - accuracy: 0.5349 - f1_score: 0.5942\n",
      "Epoch 5/10\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.6920 - accuracy: 0.5222 - f1_score: 0.5415\n",
      "Epoch 6/10\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.6904 - accuracy: 0.5291 - f1_score: 0.6125\n",
      "Epoch 7/10\n",
      "58/58 [==============================] - 0s 8ms/step - loss: 0.6898 - accuracy: 0.5424 - f1_score: 0.6088\n",
      "Epoch 8/10\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.6896 - accuracy: 0.5430 - f1_score: 0.6036\n",
      "Epoch 9/10\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.6898 - accuracy: 0.5384 - f1_score: 0.6183\n",
      "Epoch 10/10\n",
      "58/58 [==============================] - 1s 9ms/step - loss: 0.6890 - accuracy: 0.5401 - f1_score: 0.5981\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.5041 - f1_score: 0.6554\n",
      "Loss for split: [0.6863299608230591, 0.5040650367736816, array([0.65536726], dtype=float32)]\n",
      "Epoch 1/10\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 0.6898 - accuracy: 0.5366 - f1_score: 0.6312\n",
      "Epoch 2/10\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 0.6895 - accuracy: 0.5318 - f1_score: 0.6194\n",
      "Epoch 3/10\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 0.6891 - accuracy: 0.5323 - f1_score: 0.6149\n",
      "Epoch 4/10\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 0.6895 - accuracy: 0.5436 - f1_score: 0.6409\n",
      "Epoch 5/10\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 0.6902 - accuracy: 0.5366 - f1_score: 0.6000\n",
      "Epoch 6/10\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 0.6892 - accuracy: 0.5399 - f1_score: 0.6215\n",
      "Epoch 7/10\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 0.6897 - accuracy: 0.5356 - f1_score: 0.6028\n",
      "Epoch 8/10\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 0.6889 - accuracy: 0.5393 - f1_score: 0.6471\n",
      "Epoch 9/10\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 0.6884 - accuracy: 0.5393 - f1_score: 0.6258\n",
      "Epoch 10/10\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 0.6887 - accuracy: 0.5366 - f1_score: 0.6290\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6827 - accuracy: 0.6341 - f1_score: 0.7761\n",
      "Loss for split: [0.6826902627944946, 0.6341463327407837, array([0.7761194], dtype=float32)]\n",
      "Epoch 1/10\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.6877 - accuracy: 0.5387 - f1_score: 0.6073\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.6875 - accuracy: 0.5361 - f1_score: 0.6450\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.6874 - accuracy: 0.5412 - f1_score: 0.6545\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.6876 - accuracy: 0.5467 - f1_score: 0.6402\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.6869 - accuracy: 0.5462 - f1_score: 0.6473\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.6861 - accuracy: 0.5442 - f1_score: 0.6501\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.6872 - accuracy: 0.5447 - f1_score: 0.6008\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 1s 10ms/step - loss: 0.6872 - accuracy: 0.5442 - f1_score: 0.6606\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.6866 - accuracy: 0.5392 - f1_score: 0.6259\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 1s 9ms/step - loss: 0.6860 - accuracy: 0.5478 - f1_score: 0.6464\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.6974 - accuracy: 0.5122 - f1_score: 0.6774\n",
      "Loss for split: [0.6973982453346252, 0.5121951103210449, array([0.67741936], dtype=float32)]\n",
      "Epoch 1/10\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.6863 - accuracy: 0.5452 - f1_score: 0.6472\n",
      "Epoch 2/10\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.6858 - accuracy: 0.5561 - f1_score: 0.6558\n",
      "Epoch 3/10\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.6870 - accuracy: 0.5366 - f1_score: 0.6456\n",
      "Epoch 4/10\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.6861 - accuracy: 0.5433 - f1_score: 0.6187\n",
      "Epoch 5/10\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.6863 - accuracy: 0.5476 - f1_score: 0.6510\n",
      "Epoch 6/10\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.6867 - accuracy: 0.5352 - f1_score: 0.6443\n",
      "Epoch 7/10\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.6855 - accuracy: 0.5400 - f1_score: 0.6185\n",
      "Epoch 8/10\n",
      "71/71 [==============================] - 1s 9ms/step - loss: 0.6847 - accuracy: 0.5433 - f1_score: 0.6316\n",
      "Epoch 9/10\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.6859 - accuracy: 0.5423 - f1_score: 0.6201\n",
      "Epoch 10/10\n",
      "71/71 [==============================] - 1s 10ms/step - loss: 0.6853 - accuracy: 0.5333 - f1_score: 0.6163\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6702 - accuracy: 0.6179 - f1_score: 0.7117\n",
      "Loss for split: [0.6702284812927246, 0.6178861856460571, array([0.71165645], dtype=float32)]\n",
      "Epoch 1/10\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.6841 - accuracy: 0.5497 - f1_score: 0.6487\n",
      "Epoch 2/10\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.6842 - accuracy: 0.5510 - f1_score: 0.6589\n",
      "Epoch 3/10\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.6846 - accuracy: 0.5465 - f1_score: 0.6297\n",
      "Epoch 4/10\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.6829 - accuracy: 0.5542 - f1_score: 0.6470\n",
      "Epoch 5/10\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.6829 - accuracy: 0.5591 - f1_score: 0.6607\n",
      "Epoch 6/10\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.6830 - accuracy: 0.5560 - f1_score: 0.6511\n",
      "Epoch 7/10\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.6834 - accuracy: 0.5542 - f1_score: 0.6437\n",
      "Epoch 8/10\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.6824 - accuracy: 0.5627 - f1_score: 0.6558\n",
      "Epoch 9/10\n",
      "75/75 [==============================] - 1s 10ms/step - loss: 0.6815 - accuracy: 0.5537 - f1_score: 0.6591\n",
      "Epoch 10/10\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 0.6821 - accuracy: 0.5618 - f1_score: 0.6471\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6632 - accuracy: 0.6098 - f1_score: 0.7419\n",
      "Loss for split: [0.6632305383682251, 0.6097561120986938, array([0.7419355], dtype=float32)]\n",
      "Epoch 1/10\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.6806 - accuracy: 0.5647 - f1_score: 0.6762\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.6802 - accuracy: 0.5596 - f1_score: 0.6665\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.6806 - accuracy: 0.5579 - f1_score: 0.6588\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.6793 - accuracy: 0.5630 - f1_score: 0.6806\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.6791 - accuracy: 0.5639 - f1_score: 0.6682\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.6789 - accuracy: 0.5545 - f1_score: 0.6634\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.6792 - accuracy: 0.5592 - f1_score: 0.6677\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.6792 - accuracy: 0.5618 - f1_score: 0.6665\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.6784 - accuracy: 0.5639 - f1_score: 0.6755\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 1s 10ms/step - loss: 0.6781 - accuracy: 0.5677 - f1_score: 0.6762\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7583 - accuracy: 0.4959 - f1_score: 0.6630\n",
      "Loss for split: [0.7583115696907043, 0.49593496322631836, array([0.6630435], dtype=float32)]\n",
      "Epoch 1/10\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 0.6821 - accuracy: 0.5637 - f1_score: 0.6627\n",
      "Epoch 2/10\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 0.6829 - accuracy: 0.5520 - f1_score: 0.6671\n",
      "Epoch 3/10\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 0.6819 - accuracy: 0.5658 - f1_score: 0.6691\n",
      "Epoch 4/10\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 0.6802 - accuracy: 0.5641 - f1_score: 0.6776\n",
      "Epoch 5/10\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 0.6807 - accuracy: 0.5645 - f1_score: 0.6761\n",
      "Epoch 6/10\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 0.6820 - accuracy: 0.5694 - f1_score: 0.6826\n",
      "Epoch 7/10\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 0.6796 - accuracy: 0.5743 - f1_score: 0.6797\n",
      "Epoch 8/10\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 0.6816 - accuracy: 0.5629 - f1_score: 0.6606\n",
      "Epoch 9/10\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 0.6804 - accuracy: 0.5565 - f1_score: 0.6689\n",
      "Epoch 10/10\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 0.6796 - accuracy: 0.5662 - f1_score: 0.6750\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6861 - accuracy: 0.5772 - f1_score: 0.7320\n",
      "Loss for split: [0.686082661151886, 0.577235758304596, array([0.7319588], dtype=float32)]\n",
      "Epoch 1/10\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.6809 - accuracy: 0.5628 - f1_score: 0.6717\n",
      "Epoch 2/10\n",
      "87/87 [==============================] - 1s 11ms/step - loss: 0.6795 - accuracy: 0.5698 - f1_score: 0.6858\n",
      "Epoch 3/10\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.6797 - accuracy: 0.5632 - f1_score: 0.6738\n",
      "Epoch 4/10\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.6810 - accuracy: 0.5582 - f1_score: 0.6699\n",
      "Epoch 5/10\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.6788 - accuracy: 0.5709 - f1_score: 0.6806\n",
      "Epoch 6/10\n",
      "87/87 [==============================] - 1s 9ms/step - loss: 0.6789 - accuracy: 0.5721 - f1_score: 0.6779\n",
      "Epoch 7/10\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.6794 - accuracy: 0.5771 - f1_score: 0.6900\n",
      "Epoch 8/10\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.6783 - accuracy: 0.5705 - f1_score: 0.6815\n",
      "Epoch 9/10\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.6788 - accuracy: 0.5786 - f1_score: 0.6836\n",
      "Epoch 10/10\n",
      "87/87 [==============================] - 1s 10ms/step - loss: 0.6781 - accuracy: 0.5709 - f1_score: 0.6782\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7129 - accuracy: 0.5122 - f1_score: 0.6774\n",
      "Loss for split: [0.712902843952179, 0.5121951103210449, array([0.67741936], dtype=float32)]\n",
      "Epoch 1/10\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.6794 - accuracy: 0.5694 - f1_score: 0.6777\n",
      "Epoch 2/10\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.6780 - accuracy: 0.5605 - f1_score: 0.6652\n",
      "Epoch 3/10\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6793 - accuracy: 0.5620 - f1_score: 0.6655\n",
      "Epoch 4/10\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6810 - accuracy: 0.5686 - f1_score: 0.6650\n",
      "Epoch 5/10\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6785 - accuracy: 0.5609 - f1_score: 0.6704\n",
      "Epoch 6/10\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6778 - accuracy: 0.5716 - f1_score: 0.6806\n",
      "Epoch 7/10\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6778 - accuracy: 0.5738 - f1_score: 0.6822\n",
      "Epoch 8/10\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6768 - accuracy: 0.5749 - f1_score: 0.6758\n",
      "Epoch 9/10\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6772 - accuracy: 0.5720 - f1_score: 0.6765\n",
      "Epoch 10/10\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6759 - accuracy: 0.5716 - f1_score: 0.6706\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.7384 - accuracy: 0.5041 - f1_score: 0.4959\n",
      "Loss for split: [0.7383719682693481, 0.5040650367736816, array([0.49586776], dtype=float32)]\n",
      "Epoch 1/10\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 0.6885 - accuracy: 0.5482 - f1_score: 0.6537\n",
      "Epoch 2/10\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 0.6835 - accuracy: 0.5535 - f1_score: 0.6549\n",
      "Epoch 3/10\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 0.6817 - accuracy: 0.5623 - f1_score: 0.6634\n",
      "Epoch 4/10\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 0.6807 - accuracy: 0.5704 - f1_score: 0.6748\n",
      "Epoch 5/10\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 0.6814 - accuracy: 0.5585 - f1_score: 0.6643\n",
      "Epoch 6/10\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 0.6788 - accuracy: 0.5690 - f1_score: 0.6621\n",
      "Epoch 7/10\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 0.6805 - accuracy: 0.5694 - f1_score: 0.6786\n",
      "Epoch 8/10\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 0.6790 - accuracy: 0.5697 - f1_score: 0.6674\n",
      "Epoch 9/10\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 0.6790 - accuracy: 0.5718 - f1_score: 0.6800\n",
      "Epoch 10/10\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 0.6773 - accuracy: 0.5729 - f1_score: 0.6751\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7003 - accuracy: 0.5366 - f1_score: 0.4673\n",
      "Loss for split: [0.7002695798873901, 0.5365853905677795, array([0.46728972], dtype=float32)]\n",
      "Epoch 1/10\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 0.6894 - accuracy: 0.5515 - f1_score: 0.6425\n",
      "Epoch 2/10\n",
      "99/99 [==============================] - 1s 11ms/step - loss: 0.6847 - accuracy: 0.5592 - f1_score: 0.6602\n",
      "Epoch 3/10\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 0.6828 - accuracy: 0.5677 - f1_score: 0.6630\n",
      "Epoch 4/10\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 0.6820 - accuracy: 0.5653 - f1_score: 0.6648\n",
      "Epoch 5/10\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 0.6840 - accuracy: 0.5626 - f1_score: 0.6482\n",
      "Epoch 6/10\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 0.6833 - accuracy: 0.5629 - f1_score: 0.6635\n",
      "Epoch 7/10\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 0.6813 - accuracy: 0.5640 - f1_score: 0.6573\n",
      "Epoch 8/10\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 0.6800 - accuracy: 0.5673 - f1_score: 0.6625\n",
      "Epoch 9/10\n",
      "99/99 [==============================] - 1s 9ms/step - loss: 0.6797 - accuracy: 0.5680 - f1_score: 0.6524\n",
      "Epoch 10/10\n",
      "99/99 [==============================] - 1s 10ms/step - loss: 0.6816 - accuracy: 0.5650 - f1_score: 0.6426\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6842 - accuracy: 0.5935 - f1_score: 0.7423\n",
      "Loss for split: [0.6841813325881958, 0.5934959053993225, array([0.7422681], dtype=float32)]\n",
      "Epoch 1/10\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6829 - accuracy: 0.5651 - f1_score: 0.6719\n",
      "Epoch 2/10\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6813 - accuracy: 0.5616 - f1_score: 0.6647\n",
      "Epoch 3/10\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6792 - accuracy: 0.5797 - f1_score: 0.6779\n",
      "Epoch 4/10\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6808 - accuracy: 0.5693 - f1_score: 0.6743\n",
      "Epoch 5/10\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6781 - accuracy: 0.5719 - f1_score: 0.6718\n",
      "Epoch 6/10\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6777 - accuracy: 0.5781 - f1_score: 0.6796\n",
      "Epoch 7/10\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6785 - accuracy: 0.5732 - f1_score: 0.6693\n",
      "Epoch 8/10\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6787 - accuracy: 0.5658 - f1_score: 0.6623\n",
      "Epoch 9/10\n",
      "103/103 [==============================] - 1s 10ms/step - loss: 0.6809 - accuracy: 0.5593 - f1_score: 0.6480\n",
      "Epoch 10/10\n",
      "103/103 [==============================] - 1s 9ms/step - loss: 0.6786 - accuracy: 0.5726 - f1_score: 0.6710\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7010 - accuracy: 0.4634 - f1_score: 0.5926\n",
      "Loss for split: [0.7009996771812439, 0.46341463923454285, array([0.59259266], dtype=float32)]\n",
      "Epoch 1/10\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6829 - accuracy: 0.5478 - f1_score: 0.6577\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.6875 - accuracy: 0.5316 - f1_score: 0.6335\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6845 - accuracy: 0.5457 - f1_score: 0.6522\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.6810 - accuracy: 0.5578 - f1_score: 0.6724\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6787 - accuracy: 0.5668 - f1_score: 0.6709\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 1s 10ms/step - loss: 0.6777 - accuracy: 0.5678 - f1_score: 0.6726\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6803 - accuracy: 0.5665 - f1_score: 0.6616\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6781 - accuracy: 0.5665 - f1_score: 0.6763\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6776 - accuracy: 0.5709 - f1_score: 0.6697\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - 1s 9ms/step - loss: 0.6783 - accuracy: 0.5656 - f1_score: 0.6644\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6999 - accuracy: 0.4797 - f1_score: 0.5676\n",
      "Loss for split: [0.6999378800392151, 0.4796747863292694, array([0.5675676], dtype=float32)]\n",
      "Epoch 1/10\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.6808 - accuracy: 0.5516 - f1_score: 0.6546\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.6791 - accuracy: 0.5663 - f1_score: 0.6559\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.6783 - accuracy: 0.5630 - f1_score: 0.6650\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.6785 - accuracy: 0.5609 - f1_score: 0.6554\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.6783 - accuracy: 0.5618 - f1_score: 0.6584\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.6777 - accuracy: 0.5672 - f1_score: 0.6685\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.6783 - accuracy: 0.5621 - f1_score: 0.6517\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 1s 9ms/step - loss: 0.6763 - accuracy: 0.5678 - f1_score: 0.6736\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.6776 - accuracy: 0.5675 - f1_score: 0.6580\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 1s 10ms/step - loss: 0.6770 - accuracy: 0.5735 - f1_score: 0.6648\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6989 - accuracy: 0.5366 - f1_score: 0.6587\n",
      "Loss for split: [0.6989307403564453, 0.5365853905677795, array([0.6586826], dtype=float32)]\n",
      "Epoch 1/10\n",
      "116/116 [==============================] - 1s 9ms/step - loss: 0.6807 - accuracy: 0.5557 - f1_score: 0.6588\n",
      "Epoch 2/10\n",
      "116/116 [==============================] - 1s 9ms/step - loss: 0.6783 - accuracy: 0.5690 - f1_score: 0.6623\n",
      "Epoch 3/10\n",
      "116/116 [==============================] - 1s 9ms/step - loss: 0.6778 - accuracy: 0.5676 - f1_score: 0.6551\n",
      "Epoch 4/10\n",
      "116/116 [==============================] - 1s 9ms/step - loss: 0.6764 - accuracy: 0.5737 - f1_score: 0.6647\n",
      "Epoch 5/10\n",
      "116/116 [==============================] - 1s 10ms/step - loss: 0.6775 - accuracy: 0.5673 - f1_score: 0.6734\n",
      "Epoch 6/10\n",
      "116/116 [==============================] - 1s 9ms/step - loss: 0.6778 - accuracy: 0.5624 - f1_score: 0.6644\n",
      "Epoch 7/10\n",
      "116/116 [==============================] - 1s 9ms/step - loss: 0.6779 - accuracy: 0.5635 - f1_score: 0.6655\n",
      "Epoch 8/10\n",
      "116/116 [==============================] - 1s 9ms/step - loss: 0.6810 - accuracy: 0.5598 - f1_score: 0.6594\n",
      "Epoch 9/10\n",
      "116/116 [==============================] - 1s 10ms/step - loss: 0.6755 - accuracy: 0.5661 - f1_score: 0.6589\n",
      "Epoch 10/10\n",
      "116/116 [==============================] - 1s 10ms/step - loss: 0.6763 - accuracy: 0.5754 - f1_score: 0.6691\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6856 - accuracy: 0.5041 - f1_score: 0.6433\n",
      "Loss for split: [0.6855932474136353, 0.5040650367736816, array([0.64327484], dtype=float32)]\n",
      "Epoch 1/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.6756 - accuracy: 0.5785 - f1_score: 0.6716\n",
      "Epoch 2/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.6763 - accuracy: 0.5654 - f1_score: 0.6508\n",
      "Epoch 3/10\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.6771 - accuracy: 0.5729 - f1_score: 0.6710\n",
      "Epoch 4/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.6758 - accuracy: 0.5682 - f1_score: 0.6613\n",
      "Epoch 5/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.6748 - accuracy: 0.5735 - f1_score: 0.6739\n",
      "Epoch 6/10\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 0.6741 - accuracy: 0.5676 - f1_score: 0.6552\n",
      "Epoch 7/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.6748 - accuracy: 0.5626 - f1_score: 0.6591\n",
      "Epoch 8/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.6758 - accuracy: 0.5690 - f1_score: 0.6623\n",
      "Epoch 9/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.6744 - accuracy: 0.5671 - f1_score: 0.6649\n",
      "Epoch 10/10\n",
      "120/120 [==============================] - 1s 9ms/step - loss: 0.6746 - accuracy: 0.5777 - f1_score: 0.6675\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6918 - accuracy: 0.5854 - f1_score: 0.6982\n",
      "Loss for split: [0.6918201446533203, 0.5853658318519592, array([0.69822484], dtype=float32)]\n",
      "Epoch 1/10\n",
      "124/124 [==============================] - 1s 9ms/step - loss: 0.6749 - accuracy: 0.5717 - f1_score: 0.6708\n",
      "Epoch 2/10\n",
      "124/124 [==============================] - 1s 9ms/step - loss: 0.6747 - accuracy: 0.5761 - f1_score: 0.6605\n",
      "Epoch 3/10\n",
      "124/124 [==============================] - 1s 10ms/step - loss: 0.6738 - accuracy: 0.5766 - f1_score: 0.6731\n",
      "Epoch 4/10\n",
      "124/124 [==============================] - 1s 9ms/step - loss: 0.6726 - accuracy: 0.5717 - f1_score: 0.6729\n",
      "Epoch 5/10\n",
      "124/124 [==============================] - 1s 9ms/step - loss: 0.6748 - accuracy: 0.5704 - f1_score: 0.6644\n",
      "Epoch 6/10\n",
      "124/124 [==============================] - 1s 9ms/step - loss: 0.6750 - accuracy: 0.5712 - f1_score: 0.6658\n",
      "Epoch 7/10\n",
      "124/124 [==============================] - 1s 9ms/step - loss: 0.6728 - accuracy: 0.5796 - f1_score: 0.6802\n",
      "Epoch 8/10\n",
      "124/124 [==============================] - 1s 9ms/step - loss: 0.6746 - accuracy: 0.5758 - f1_score: 0.6700\n",
      "Epoch 9/10\n",
      "124/124 [==============================] - 1s 9ms/step - loss: 0.6739 - accuracy: 0.5736 - f1_score: 0.6703\n",
      "Epoch 10/10\n",
      "124/124 [==============================] - 1s 9ms/step - loss: 0.6731 - accuracy: 0.5731 - f1_score: 0.6661\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.7104 - accuracy: 0.4715 - f1_score: 0.6409\n",
      "Loss for split: [0.7104058265686035, 0.47154471278190613, array([0.640884], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.metrics import F1Score\n",
    "\n",
    "\n",
    "# Préparation des données\n",
    "X = features.values\n",
    "y = target.values\n",
    "y = target.values.astype(np.float32)\n",
    "\n",
    "\n",
    "# Définition du modèle LSTM\n",
    "model = keras.Sequential([\n",
    "    layers.InputLayer(input_shape=(X.shape[1], 1)),  # X.shape[1] : nombre de features\n",
    "    layers.LSTM(16, return_sequences=True),  # *** : nombre d'unités pour la première couche LSTM\n",
    "    layers.LSTM(8),  # *** : nombre d'unités pour la deuxième couche LSTM\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilation du modèle avec la métrique F1\n",
    "f1_metric = F1Score(threshold=0.5)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_metric])\n",
    "\n",
    "# TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=30)  # *** : nombre de splits\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    # 1. Séparation des données\n",
    "    X_train_raw, X_test_raw = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # 2. Normalisation des données\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "    X_test_scaled = scaler.transform(X_test_raw)\n",
    "    \n",
    "    # 3. Reshape pour LSTM\n",
    "    X_train = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)\n",
    "    X_test = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)\n",
    "    \n",
    "    # Entraînement du modèle\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=30)\n",
    "    \n",
    "    # Évaluation du modèle sur le sous-ensemble de test\n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "    print(f\"Loss for split: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
